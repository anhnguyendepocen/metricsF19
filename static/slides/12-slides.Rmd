---
title: "2.6: Inference for Regression"
subtitle: "ECON 480 · Econometrics · Fall 2019"
author: 'Ryan Safner<br> Assistant Professor of Economics <br> <a href="mailto:safner@hood.edu"><i class="fa fa-paper-plane fa-fw"></i> safner@hood.edu</a> <br> <a href="https://github.com/ryansafner/metricsf19"><i class="fa fa-github fa-fw"></i> ryansafner/metricsf19</a><br> <a href="https://metricsF19.classes.ryansafner.com"> <i class="fa fa-globe fa-fw"></i> metricsF19.classes.ryansafner.com</a><br>'
#date:
output:
  xaringan::moon_reader:
    mathjax: "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_HTMLorMML" # rescales math with css changes https://github.com/yihui/xaringan/issues/143
    lib_dir: libs
    df_print: paged
    #seal: false
    css: [custom.css, custom-fonts.css, "hygge"] #, metropolis, metropolis-fonts
    nature:
      beforeInit: ["macros.js", "https://platform.twitter.com/widgets.js"] # first is for rescaling images , second is for embedding tweets, https://github.com/yihui/xaringan/issues/100
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ratio: "16:9"
      slideNumberFormat: |
        <div class="progress-bar-container">
          <div class="progress-bar" style="width: calc(%current% / %total% * 100%);">
          </div>
        </div>
    includes:
      in_header: header.html # for font awesome, used in title  
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(echo=F,
                      message=F,
                      warning=F)
library(tidyverse)
set.seed(256)
update_geom_defaults("label", list(family = "Fira Sans Condensed"))
```

```{r regression-setup, echo=F, results="hide"}

library(haven)
CASchool<-read_dta("../data/caschool.dta")

# run regression of testscr on str
school_reg <- lm(testscr ~ str, 
                 data = CASchool)

library(broom)
school_reg_tidy <- tidy(school_reg,
     conf.int = TRUE) # add confidence intervals
CASchool_aug <- augment(school_reg)

library(equatiomatic)
extract_eq(school_reg, use_coefs = TRUE,coef_digits = 2,fix_signs = TRUE)

```


# The Sampling Distribution of $\hat{\beta_1}$

.pull-left[

$$\hat{\beta_1} \sim N(E[\hat{\beta_1}], \sigma_{\hat{\beta_1}})$$

1. $E[\hat{\beta_1}]$; the **center** of the distribution (2 classes ago)
    - $E[\hat{\beta_1}]=\beta_1$<sup><span class="red">1</span></sup>

2. $\sigma_{\hat{\beta_1}}$; how **precise** is our estimate? (last class)
    - Variance $\sigma^2_{\hat{\beta_1}}$ or standard error $\sigma_{\hat{\beta_1}}$


]

.pull-right[

```{r, fig.retina=3}
ggplot(data = tibble(x=-4:4))+
  aes(x = x)+
  stat_function(fun = dnorm, size=2, color="blue")+
  geom_label(x=1, y=dnorm(1), label=expression(sigma[hat(beta[1])]==1), color="blue")+
  geom_segment(aes(x=0,xend=0, y=0, yend=0.4), linetype="dashed")+
  scale_x_continuous(breaks = 0,
                     labels = expression(E(hat(beta[1]))))+
  labs(x = expression(hat(beta[1])),
       y = "Probability")+
  theme_classic(base_family = "Fira Sans Condensed",
           base_size=20)+
  stat_function(fun = dnorm, args=list(mean = 0, sd = 2), size=2, color="red")+
  geom_label(x=2, y=dnorm(2,0,2), label=expression(sigma[hat(beta[1])]==2), color="red")

```

]
.footnote[<sup>.red[1]</sup> Under the 4 assumptions about $u$ (particularly, $cor(X,u)=0)$.
]

---

# Recall: The Two Big Problems with Data

.pull-left[

- We use econometrics to .onfire[identify] causal relationships and make .onfire[inferences] about them

1. Problem for .onfire[identification]: .shout[endogeneity]
    - $X$ is **exogenous** if its variation is *unrelated* to other factors $(u)$ that affect $Y$
    - $X$ is **endogenous** if its variation is *related* to other factors $(u)$ that affect $Y$

2. Problem for .onfire[inference]: .shout[randomness]
    - Data is random due to **natural sampling variation**
    - Taking one sample of a population will yield slightly different information than another sample of the same population

]

.pull-right[
.center[
![:scale 55%](https://www.dropbox.com/s/v5vwsadw5vs448t/causality.jpg?raw=1)

![:scale 55%](https://www.dropbox.com/s/phhet82tcnuegxp/randomimage.jpg?raw=1)]
]

---

# Recall: Distributions of the OLS Estimators

- OLS estimators $(\hat{\beta_0}$ and $\hat{\beta_1})$ are computed from a finite (specific) sample of data

- Our OLS model contains **2 sources of randomness**:

--

- .onfire[*Modeled* randomness]: $u$ includes all factors affecting $Y$ *other* than $X$
    - different samples will have different values of those other factors $(u_i)$

--

- .onfire[*Sampling* randomness]: different samples will generate different OLS estimators
    - Thus, $\hat{\beta_0}, \hat{\beta_1}$ are *also* **random variables**, with their own <span class=shout>sampling distribution</span>

---

# Recall: Inferential Statistics and Sampling Distributions

.pull-left[

- **Inferential statistics** analyzes a **sample** to make inferences about a much larger (unobservable) **population**

- .shout[Population]: all possible individuals that match some well-defined criterion of interest (people, firms, cities, etc)
  - Characteristics about (relationships between variables in) populations are called **parameters** 

- .shout[Sample]: some portion of the population of interest to *represent the whole*
  - Samples generate **statistics** used to *estimate* population parameters
  
]

.pull-right[
.center[
![](https://www.dropbox.com/s/17cb7ceqipgi8fs/citymodel.jpg?raw=1)
]
]

---

# Recall: Inference in Econometrics: The Big Picture

$$\text{Sample} \underbrace{\rightarrow}_{\text{statistical inference}} \text{Population} \underbrace{\rightarrow}_{\text{causal indentification}} \text{Unobserved Parameters}$$

- We want to .onfire[identify] causal relationships between **population** variables
    - Logically first thing to consider
    - **Endogeneity problem**

- We'll use **sample** *statistics* to .onfire[infer] something about population *parameters*
    - In practice, we'll only ever have a finite *sample distribution* of data
    - We *don't* know the *population distribution* of data
    - **Randomness problem**

---

# Two Methods of Statistical Inference

1. .shout[Estimation]: use our sample data to construct a point estimate of a population parameter and subject it to a .shout[hypothesis test] 

--

2. .shout[Confidence interval]: use our sample data to construct a *range* for the population parameter

--

- First method is more common, but second is still widely acknowledged

- Both will give you similar results
    - Tradeoff of accuracy vs. precision

- Note *statistical* inference is different than *causal* inference!

---

class: inverse, center, middle

# Hypothesis Testing

---

# Estimation and Hypothesis Testing I

- We have already used statistics to **estimate** a relationship between $X$ and $Y$
    - OLS estimators $\hat{\beta_0}$ and $\hat{\beta_1}$ of the true population $\beta_0$ and $\beta_1$

- We want to **test** if these estimates are **statistically significant** and they describe the population
    - This is the "bread and butter" of inferential statistics and the purpose of regression

--

.content-box-green[
.green[**Examples**]:
- Does reducing class size actually improve test scores?
- Do more years of education increase your wages? 
- Is the gender wage gap between men and women really $0.77? 
]

--

- .onfire[All modern science is built upon statistical hypothesis testing, so understand it well!]

---

# Estimation and Hypothesis Testing II

- Note, we can test a lot of hypotheses about a lot of population parameters, e.g.
    - A population mean $\mu$ 
      - <span class="green">**Example**: average height of adults</span>
    - A population proportion $p$
      - <span class="green">**Example**: percent of voters who voted for Trump</span>
    - A difference in population means $\mu_A-\mu_B$
      - <span class="green">**Example**: difference in average wages of men vs. women</span>
    - A difference in population proportions $p_A-p_B$
      - <span class="green">**Example**: difference in percent of patients reporting symptoms of drug A vs B</span>
    - See all the possibilities in glorious detail in today's Class Notes

- We will focus only on hypotheses about the .onfire[population regression slope] $(\hat{\beta}_1)$, i.e. the .onfire[causal effect]<sup>.red[1]</sup> of $X$ on $Y$

.footnote[<sup>1</sup> With a model this simple, it's almost certainly **not** causal, but this is the ultimate direction we are heading...]

---

# Null and Alternative Hypotheses I

- All scientific inquiries begin with a .shout[null hypothesis] $(H_0)$ that proposes a specific value of a population parameter
    - Notation: add a subscript 0: $\beta_{1,0}$ (or $\mu_0$, $p_0$, etc)

--

- We suggest an .shout[alternative hypothesis] $(H_a)$, often the one we hope to verify
    - Note, can be multiple alternative hypotheses: $H_1, H_2, \ldots , H_n$

--

- Ask: .onfire["Does our data (sample) give us sufficient evidence to reject `\\(H_0\\)` in favor of `\\(H_a\\)`?"]
    - Note: **the test is *always* about** $\mathbf{H_0}$! 
    - See if we have sufficient evidence to reject the status quo

---

# Null and Alternative Hypotheses II

- Null hypothesis assigns a value (or a range) to a population parameter
    - e.g. $\beta_1=2$ or $\beta_1 \leq 20$
    - **Most common null hypothesis is $\beta_1=0$** $\implies$ $X$ has no effect on $Y$ (no slope for a line)
    - Note: always an equality!

--

- Alternative hypothesis must mathematically *contradict* the null hypothesis
    - e.g. $\beta_1 \neq 2$ or $\beta_1 > 20$ or $\beta_1 \neq 0$
    - Note: always an inequality!

--

- Alternative hypotheses come in two forms:
    1. **One-sided alternative**: $\beta_1 >H_0$ or $\beta_1< H_0$
    2. **Two-sided alternative**: $\beta_1 \neq H_0$
        - Note this means either $\beta_1 < H_0$ or $\beta_1 > H_0$

---

# Components of a Valid Hypothesis Test

- All statistical hypothesis tests have the following components:

1. A .onfire[null hypothesis, `\\(H_0\\)`]

2. An .onfire[alternative hypothesis, `\\(H_a\\)`]

3. A .onfire[test statistic] to determine if we reject $H_0$ when the statistic reaches a "critical value"
    - Beyond the critical value is the "rejection region", sufficient evidence to reject $H_0$
    
4. A .onfire[conclusion] whether or not to reject $H_0$ in favor of $H_a$

---

# Type I and Type II Errors I

.pull-left[

- Any sample statistic (e.g. $\hat{\beta_1}$) will rarely be exactly equal to the hypothesized population parameter (e.g. $\beta_1$)

- Difference between observed statistic and true paremeter could be because:

1. Parameter is *not* the hypothesized value $(H_0$ is *false*)
    
2. Parameter is truly the hypothesized value $(H_0$ is *true*) but *sampling variability* gave us a different estimate 
    
- .onfire[We cannot distinguish between these two possibilities with any certainty]

]

.pull-right[
.center[
![](https://www.dropbox.com/s/95rh9ow982y5htb/error.png?raw=1)
]
]
---

# Type I and Type II Errors II

.pull-left[

- We can interpret our estimates probabilistically as commiting one of two types of error:

1. .shout[Type I error (false positive)]: rejecting $H_0$ when it is in fact true
    - Believing we found an important result when there is truly no relationship
    
2. .shout[Type II error (false negative)]: failing to reject $H_0$ when it is in fact false
    - Believing we found nothing when there was truly a relationship to find
]

.pull-right[
.center[
![](https://www.dropbox.com/s/95rh9ow982y5htb/error.png?raw=1)
]
]

---

# Type I and Type II Errors III

```{r, echo=FALSE, results="asis", eval=F}
library(kableExtra)
tribble(
  ~"", ~"", ~"Null is True", ~"Null is False",
  "Judgment", "Reject Null", "Type I Error", "Correct",
  "Judgment", "Don't Reject Null", "Correct", "Type II Error"
  ) %>%
  knitr::kable(.,format="html") %>%
  kable_styling(full_width = F) %>%
  add_header_above(c(" " = 2, "Truth" = 2)) %>%
  column_spec(c(1,2), bold=T) %>%
  collapse_rows(columns = 1, latex_hline = "major", valign = "middle")
```

.regtable[
```{r, echo=FALSE, results="asis"}
library(kableExtra)
tribble(
  ~"", ~"", ~"Null is True", ~"Null is False",
  "Judgment", "Reject Null", "TYPE I ERROR", "CORRECT",
  "Judgment", "Reject Null", "(False +)", "(True +)",
  "Judgment", "Don't Reject Null", "CORRECT", "TYPE II ERROR",
  "Judgment", "Don't Reject Null", "(True -)", "(False -)"
  ) %>%
  knitr::kable(.,format="html") %>%
  kable_styling(full_width = F) %>%
  add_header_above(c(" " = 2, "Truth" = 2)) %>%
  column_spec(c(1,2), bold=T) %>%
  collapse_rows(columns = c(1,2), latex_hline = "major", valign = "middle")

```
]

- Depending on context, committing one type of error may be more serious than the other

---

# Type I and Type II Errors IV

.regtable[
```{r, echo=FALSE, results="asis"}
library(kableExtra)
tribble(
  ~"", ~"", ~"Defendant is Innocent", ~"Defendant is Guilty",
  "Judgment", "Convict", "TYPE I ERROR", "CORRECT",
  "Judgment", "Convict", "(False +)", "(True +)",
  "Judgment", "Acquit", "CORRECT", "TYPE II ERROR",
  "Judgment", "Acquit", "(True -)", "(False -)"
  ) %>%
  knitr::kable(.,format="html") %>%
  kable_styling(full_width = F) %>%
  add_header_above(c(" " = 2, "Truth" = 2)) %>%
  column_spec(c(1,2), bold=T) %>%
  collapse_rows(columns = c(1,2), latex_hline = "major", valign = "middle")

```
]

- Anglo-American common law *presumes* defendant is innocent: $H_0$

--

- Jury judges whether the evidence presented against the defendant is plausible *assuming the defendant were in fact innocent*

--

- If highly improbable: sufficient evidence to reject $H_0$ and convict
    - Beyond a "reasonable doubt" that the defendant is innocent

---

# Type I and Type II Errors V

.left-column[

.center[
![:scale 80%](https://www.dropbox.com/s/bufqsl9ub03iru3/blackstone.png?raw=1)

William Blackstone

(1723-1780)

]
]

.right-column[

> "It is better that ten guilty persons escape than that one innocent suffer."

- Type I error is worse than a Type II error in law!

]

.source[Blackstone, William, 1765-1770, *Commentaries on the Laws of England*]

---

# Type I and Type II Errors VI

.center[
![](https://www.dropbox.com/s/wf6i1462dadmvrw/errorspregnant.png?raw=1)
]

---

# Significance Level, $\alpha$, and Confidence Level $1-\alpha$

- The .shout[significance level, `\\(\alpha\\)`], is the probability of a **Type I error** 

$$\alpha=P(\text{Reject } H_0 | H_0 \text{ is true})$$

--

- The .shout[confidence level] is defined as .shout[`\\((1-\alpha)\\)`]
  - Specify *in advance* an $\alpha$-level (0.10, 0.05, 0.01) with associated confidence level (90%, 95%, 99%)

--

- The probability of a **Type II error** is defined as $\beta$:

$$\beta=P(\text{Don't reject } H_0 | H_0 \text{ is false})$$

---

# $\alpha$ and $\beta$

.regtable[
```{r, echo=FALSE, results="asis"}
tribble(
  ~"", ~"", ~"Null is True", ~"Null is False",
  "Judgment", "Reject Null", "TYPE I ERROR", "CORRECT",
  "Judgment", "Reject Null", "(alpha)", "(1-beta)",
  "Judgment", "Don't Reject Null", "CORRECT", "TYPE II ERROR",
  "Judgment", "Don't Reject Null", "(1-alpha)", "(beta)"
  ) %>%
  knitr::kable(.,format="html") %>%
  kable_styling(full_width = F) %>%
  add_header_above(c(" " = 2, "Truth" = 2)) %>%
  column_spec(c(1,2), bold=T) %>%
  collapse_rows(columns = c(1,2), latex_hline = "major", valign = "middle")
```
]

---

# Power and p-values

- The statistical .shout[power of the test] is $(1-\beta)$: the probability of correctly rejecting $H_0$ when $H_0$ is in fact false (e.g. not convicting an innocent person)

$$\text{Power} = 1- \beta = P(\text{Reject }H_0|H_0 \text{ is false})$$

--

- The .shout[`\\(p\\)`-value] or .shout[significance probability] is the probability that, given the null hypothesis is true, the test statistic from a random sample will be at least as extreme as the test statistic of our sample

$$p(\delta \geq \delta_i|H_0 \text{ is true})$$
  - where $\delta$ represents some test statistic
  - $\delta_i$ is the test statistic we observe in our sample
  - More on this in a bit

---

# p-Values and Statistical Significance 
 
- After running our test, we need to make a *decision* between the competing hypotheses

- Compare $p$-value with *pre-determined* $\alpha$ (commonly, $\alpha=0.05$, 95% confidence level)
    - If $p<\alpha$: **statistically significant** evidence sufficient to *reject* $H_0$ in favor of $H_a$
    - If $p \geq \alpha$: *insufficient* evidence to reject $H_0$
        - Note this does **not** mean $H_0$ is true! We merely have *failed* to *reject* $H_0$

---

class: inverse, center, middle

# Digression: p-Values and the Philosophy of Science

---

# Hypothesis Testing and the Philosophy of Science I

.left-column[
.center[
![:scale 75%](https://www.dropbox.com/s/kssn3fwxgu8k457/rafisher.jpg?raw=1)

Sir Ronald A. Fisher

(1890&mdash;1962)
]
]

.right-column[

> "The null hypothesis is never proved or established, but is possibly disproved, in the course of experimentation. Every experiment may be said to exist only in order to give the facts a chance of disproving the null hypothesis."

1931, *The Design of Experiments*
]

---

# Hypothesis Testing and the Philosophy of Science I

.pull-left[

- Modern philosophy of science is largely based off of hypothesis testing and .onfire[falsifiability], which form the "Scientific Method"<sup>.red[1]</sup>

- For something to be "scientific", it must be .onfire[falsifiable], or at least .onfire[testable]

- Hypotheses can be *corroborated* with evidence, but always *tentative* until falsified by data in suggesting an alternative hypothesis

> **"All swans are white"** is a hypothesis rejected upon discovery of a single black swan 
]

.pull-right[

.center[
.polaroid[![](https://www.dropbox.com/s/d7i7m77iui5sdb8/blackswan.jpg?raw=1)]
]
]
.footnote[<sup>1</sup> Note: economics is a very different kind of "science" with a different methodology!]

---

# Hypothesis Testing and p-Values

- Hypothesis testing, confidence intervals, and p-values are probably the hardest thing to understand in statistics

.center[

<iframe src="https://fivethirtyeight.abcnews.go.com/video/embed/56150342" width="640" height="360" scrolling="no" style="border:none;" allowfullscreen></iframe>

[Fivethirtyeight: Not Even Scientists Can Easily Explain P-values](https://fivethirtyeight.com/features/not-even-scientists-can-easily-explain-p-values/)]

---

# Hypothesis Testing: Which Test? I

- Rigorous course on statistics ([**ECMG 212**](http://ryansafner.com/courses/ecmg212) or **MATH 112**) will spend weeks going through different types of tests:
    - Sample mean; difference of means
    - Proportion; difference of proportions
    - Z-test vs t-test
    - 1 sample vs. 2 samples
    - $\chi^2$ test

- See today's [class notes](/class/12-class) page for more

---

# Hypothesis Testing: Which Test? II

.center[
![:scale 80%](https://www.dropbox.com/s/3f33cnn595x2nsj/hypothesistestflowchart.png?raw=1)
]

---

# There is Only One Test

- Fortunately, some clever statisticians realized ["**there is only one test**"](https://allendowney.blogspot.com/2011/05/there-is-only-one-test.html) and built a nice `R` package called `infer`

1. **Calculate** a statistic, $\delta_i$<sup>1</sup>, from a sample of data

2. **Simulate** a world where $\delta$ is null $(H_0)$

3. **Examine** the distribution of $\delta$ across the null world

4. **Calculate** the probability that $\delta_i$ could exist in the null world

5. **Decide** if $\delta_i$ is statistically significant

.footnote[<sup>1</sup> `\\(\delta\\)` can stand in for any test-statistic in any hypothesis test! For our purposes, `\\(\delta\\)` is the slope of our regression sample, `\\(\hat{\beta}_1\\)`.]

---

# Elements of a Hypothesis Test

.center[
![:scale 80%](https://www.dropbox.com/s/jenilao59zrbswq/onetest.png?raw=1)

[Alan Downey: "There is still only one test"](https://allendowney.blogspot.com/2016/06/there-is-still-only-one-test.html)
]

---

# Hypothesis Testing with the infer Package I

- R naturally runs the following hypothesis test on any regression as part of `lm()`:

$$\begin{align*}
H_0: \; & \beta_1=0\\
H_1: \; & \beta_1 \neq 0
\end{align*}$$

- `infer` allows you to run through these steps manually to understand the process:

--

1. `specify()` a model

--

2. `hypothesize()` the null 

--

3. `generate()` simulations of the null world 

--

4. `calculate()` the $p$-value

--

5. `visualize()` with a histogram (optional)

---

# Hypothesis Testing with the infer Package II

.center[
![:scale 65%](https://www.dropbox.com/s/zz3h7wq1tr08dus/infer0jpeg.jpeg?raw=1)
]

---

# Hypothesis Testing with the infer Package II

.center[
![:scale 65%](https://www.dropbox.com/s/25axifo95oz5zs7/infer1jpeg.jpeg?raw=1)
]

---

# Hypothesis Testing with the infer Package II

.center[
![:scale 65%](https://www.dropbox.com/s/d9bnryxznfwe4fc/infer2jpeg.jpeg?raw=1)
]

---

# Hypothesis Testing with the infer Package II

.center[
![:scale 65%](https://www.dropbox.com/s/pc8733aurnouvsr/infer3jpeg.jpeg?raw=1)
]

---

# Hypothesis Testing with the infer Package II

.center[
![:scale 65%](https://www.dropbox.com/s/4wojnmworf406ku/infer4jpeg.jpeg?raw=1)
]

---

# Hypothesis Testing with the infer Package II

.center[
![:scale 65%](https://www.dropbox.com/s/6328hg5t03jbd0w/infer5jpeg.jpeg?raw=1)
]

---

# Classical Statistical Inference: Critical Values of Test Statistic

- .onfire[Test statistic `\\((\delta)\\)`]: measures **how far what we observed in our sample $(\hat{\beta_1})$ is from what we would expect if the null hypothesis were true $(\beta_1=0)$**
    - Calculated from a sampling distribution of the estimator (i.e. $\hat{\beta_1})$
    - In econometrics, we use $t$-distributions which have $n-k-1$ degrees of freedom<sup>.red[1]</sup>

- .onfire[Rejection region]: if the test statistic reaches a .onfire["critical value"] of $\delta$, then we **reject** the null hypothesis

.footnote[<sup>.red[1]</sup> Again, see today's [class notes](/class/12-class) for more on the t-distribution. `\\(k\\)` is the number of independent variables our model has, in this case, with just one `\\(X\\)`, `\\(k=1\\)`. We use two degrees of freedom to calculate `\\(\hat{\beta_0}\\)` and `\\(\hat{\beta_1}\\)`, hence we have `\\(n-2\\)` df.]

---

class: inverse, center, middle

# Simulating the Sampling Distribution with *infer*

---

# Imagine a Null World, where $H_0$ is True

.center[
![:scale 70%](https://www.dropbox.com/s/x7b5azt5algjodg/parallelworld.jpeg?raw=1)

Our world, and a world where $\beta_1=0$ by assumption.
]

---

# Comparing the Worlds I

- From that null world where $H_0: \, \beta_1=0$ is true, we **simulate** another sample and calculate OLS estimators again

--

.pull-left[

# Our Sample

```{r, echo=F}
tidy(school_reg)
```
]

--

.pull-right[

# Another Sample

```{r, echo = F}
library(infer)
CASchool %>%
  specify(testscr ~ str) %>%
  hypothesize(null = "independence") %>%
  generate(reps = 1, type = "permute") %>%
  lm(testscr ~ str, data = .) %>%
  tidy()
```
]

---

# Comparing the Worlds II

- From that null world where $H_0: \, \beta_1=0$ is true, let's **simulate 1,000** samples and calculate slope $(\hat{\beta_1})$ for each

```{r}
CASchool %>%
  specify(testscr ~ str) %>%
  hypothesize(null = "independence") %>%
  generate(reps = 1000, type = "permute") %>%
  calculate(stat = "slope")%>%
  rename(sample = replicate,
         slope = stat)
```

---

# Prepping the *infer*  Pipeline

- Before I show you how to do this, let's first save our estimated slope from our *actual* sample
    - We'll want this later!

```{r, echo=T}
# save as obs_slope
sample_slope <- school_reg_tidy %>% # this is the regression tidied with broom
  filter(term=="str") %>%
  pull(estimate)

# confirm what it is
sample_slope
```

---
# The *infer*  Pipeline: Specify

.center[
![:scale 65%](https://www.dropbox.com/s/25axifo95oz5zs7/infer1jpeg.jpeg?raw=1)
]

---

# The *infer*  Pipeline: Specify

.left-code[
### Specify
```{r, echo=T, eval=F}
data %>%
  specify(y ~ x)
```
]

.right-plot[
- Take our data and pipe it into the `specify()` function, which is essentially a `lm()` function for regression (for our purposes)

```{r, echo=T, eval=F}
CASchool %>%
  specify(testscr ~ str)
```

```{r}
CASchool %>%
  specify(testscr ~ str)%>%
  head(.,3)
```
]

- Note nothing happens yet

---

# The *infer*  Pipeline: Hypothesize

.center[
![:scale 65%](https://www.dropbox.com/s/d9bnryxznfwe4fc/infer2jpeg.jpeg?raw=1)
]

---

# The *infer*  Pipeline: Hypothesize

.left-code[
### Specify
### Hypothesize
```{r, echo=T, eval=F}
%>% hypothesize(null = "independence")
```
]

.right-plot[
- Describe what the null hypothesis is here
- In `infer`'s language, we are hypothesizing that `str` and `testscr` are `independent` $(\beta_1=0)$<sup>.red[1]</sup>

```{r, echo=T, eval=F}
CASchool %>%
  specify(testscr ~ str) %>%
  hypothesize(null = "independence")
```


```{r}
CASchool %>%
  specify(testscr ~ str) %>%
  hypothesize(null = "independence") %>%
  head(.,3)
```

]

.footnote[<sup>.red[1]</sup> `type` can be either `point` (for specific point estimates for a single variable, such as a sample mean, `\\((\bar{x})\\)`, or `independence` (for hypotheses about two samples or a relationship between variables). See more [here](https://moderndive.netlify.com/9-hypothesis-testing.html).]

---

# The *infer*  Pipeline: Generate I

.center[
![:scale 65%](https://www.dropbox.com/s/pc8733aurnouvsr/infer3jpeg.jpeg?raw=1)
]

---

# The *infer*  Pipeline: Generate I

.left-code[
### Specify
### Hypothesize
### Generate
```{r, echo=T, eval=F}
%>% generate(reps = n,
             type = "permute")
```
]

.right-plot[
- Now the magic starts, as we run a number of simulated samples<sup>.red[1]</sup>
- Set the number of `reps` and set the `type` equal to `"permute"`

```{r, echo=T, eval=F}
i %>%
  generate(reps = 1000,
           type = "permute")
```

]

.footnote[<sup>.red[1]</sup> Note for spacing on the slide, I saved the previous code as `i` and pipe it into the remainder.]

---

# The *infer*  Pipeline: Generate II

.left-code[
### Specify
### Hypothesize
### Generate
```{r, echo=T, eval=F}
%>% generate(reps = n,
             type = "permute")
```
]

.right-plot[

```{r}
i<-CASchool %>%
  specify(testscr ~ str) %>%
  hypothesize(null = "independence")
i %>%
  generate(reps = 1000, type = "permute")
```
]

---

# The *infer*  Pipeline: Generate III

.left-code[
### Specify
### Hypothesize
### Generate
```{r, echo=T, eval=F}
%>% generate(reps = n,
             type = "permute")
```
]

.right-plot[

- There are two types of simulations we can run:<sup>.red[1]</sup>
- `"bootstrap"` takes a random draw of our *existing* sample's observations (of the same number of observations) *with replacement*
  - this approximates a sampling distribution
- `"permute"` is a `bootstrap` *without* replacement
]

.footnote[<sup>.red[1]</sup> You can do either of these in base R with `sample()`, which has 3 arguments: a vector to sample from, `size` (number of obs), and `replace` equal to `TRUE` or `FALSE`. See more for `infer`  [here](https://moderndive.netlify.com/9-hypothesis-testing.html).]

---

# The *infer*  Pipeline: Calculate I

.center[
![:scale 65%](https://www.dropbox.com/s/4wojnmworf406ku/infer4jpeg.jpeg?raw=1)
]

---

# The *infer*  Pipeline: Calculate I

.left-code[
### Specify
### Hypothesize
### Generate
### Calculate
```{r, echo=T, eval=F}
%>% calculate(stat = "")
```
]

.right-plot[
- We `calculate` sample statistics for each of the 1,000 `replicate` samples

- In our case, calculate the slope, $(\hat{\beta_1})$ for each `replicate`

```{r, echo=T, eval=F}
i %>%
  generate(reps = 1000, type = "permute") %>%
  calculate(stat = "slope")
```

- Other `stat`s for calculation: `"mean"`, `"median"`, `"prop"`, `"diff in means"`, `"diff in props"`, etc. (see [package information](https://infer.netlify.com/))
]

---

# The *infer*  Pipeline: Calculate II

.left-code[
### Specify
### Hypothesize
### Generate
### Calculate
```{r, echo=T, eval=F}
%>% calculate(stat = "")
```
]

.right-plot[
```{r, echo=F}
i %>%
  generate(reps = 1000, type = "permute") %>%
  calculate(stat = "slope")
```
]

---

# The *infer*  Pipeline: Get p Value

.left-code[
### Specify
### Hypothesize
### Generate
### Calculate
### Get p Value
```{r, echo=T, eval=F}
%>% get_p_value(obs stat = "",
                direction = "both")
```
]

.right-plot[

- We can calculate the .shout[`\\(p\\)`-value]
    - the probability of seeing a value at least as large as our `sample_slope` (`r round(sample_slope, 3)`) in our simulated null distribution

- .onfire[Two-sided alternative] $H_a: \beta_1 \neq 0$, we double the raw $p$-value

```{r, echo=F}
simulations <- i %>%
  generate(reps = 1000, type = "permute") %>%
  calculate(stat = "slope")
```

```{r, echo=T}
simulations %>%
  get_p_value(obs_stat = sample_slope,
              direction = "both")
```
]

.footnote[<sup>.red[+]</sup> Note here I saved the results of our previous code as `simulations` for spacing.]

---

# The *infer*  Pipeline: Get Confidence Interval

.left-code[
### Specify
### Hypothesize
### Generate
### Calculate
### Get confidence interval
```{r, echo=T, eval=F}
%>% get_ci(
  level = 0.95,
  type = "se",
  point_estimate = "")
```
]

.right-plot[

- We can calculate the .shout[confidence interval] for $\beta_1$ from our `sample_slope` $\hat{\beta_1}$ of `r round(sample_slope, 3)`
    - $CI_{0.95}= \hat{\beta_1} \pm 2 \times se(\hat{\beta_1})$
    - Specify confidence level $(1-\alpha)$, often $0.95$
    - Supply our original estimate for `point_estimate`

```{r, echo=T}
simulations %>%
  get_confidence_interval(level = 0.95,
                          type = "se",
                          point_estimate = sample_slope)
```
]

---

# The *infer*  Pipeline: Visualize I

.center[
![:scale 65%](https://www.dropbox.com/s/6328hg5t03jbd0w/infer5jpeg.jpeg?raw=1)
]

---

# The *infer*  Pipeline: Visualize I

.left-code[
### Specify
### Hypothesize
### Generate
### Calculate
### Visualize
```{r, echo=T, eval=F}
%>% visualize()
```
]

.right-plot[

- Make a histogram of our null distribution of $\beta_1$
    - Note it is centered at $\beta_1=0$ because that's $H_0$! 

```{r, echo=T, fig.height=4, fig.retina=3, fig.align="center"}
simulations %>%
 visualize()
```
]

---

# The *infer*  Pipeline: Visualize II

.left-code[
### Specify
### Hypothesize
### Generate
### Calculate
### Visualize
```{r, echo=T, eval=F}
%>% visualize()
```
]

.right-plot[

- Add our `sample_slope` to show our finding on the null distr.

```{r, echo=T, fig.height=4, fig.retina=3, fig.align="center"}
simulations %>%
 visualize(obs_stat = sample_slope)
```
]

---

# The *infer*  Pipeline: Visualize p-value

.left-code[
### Specify
### Hypothesize
### Generate
### Calculate
### Visualize
```{r, echo=T, eval=F}
%>% visualize()+
  shade_p_value()
```
]

.right-plot[

- Add `shade_p_value` to see what $p$ is

```{r, echo=T, fig.height=4, fig.retina=3, fig.align="center"}
simulations %>%
 visualize(obs_stat = sample_slope)+
  shade_p_value(obs_stat = sample_slope,
                direction = "two_sided")
```
]

---

# The *infer*  Pipeline: Visualize Confidence Intervals

.left-code[
### Specify
### Hypothesize
### Generate
### Calculate
### Visualize
```{r, echo=T, eval=F}
%>% visualize()+
  shade_ci()
```
]

.right-plot[

- To shade confidence interval, we first need a vector of what they are
  - I've saved the outputted `tibble` of them from 4 slides ago as `ci_values`

```{r}
ci_values<-simulations %>%
  get_confidence_interval(level = 0.95,
                          type = "se",
                          point_estimate = sample_slope)
```

```{r, echo=T, fig.height=4, fig.retina=3, fig.align="center"}
simulations %>%
 visualize(obs_stat = sample_slope)+
  shade_confidence_interval(ci_values)
```
]

---

# The *infer*  Pipeline: Visualize is a Wrapper of ggplot

- `infer`'s `visualize()` function is just a wrapper function for `ggplot()`
  - you can take your `simulations` `tibble` and just `ggplot` a normal histogram

--

.pull-left[
```{r visualize-gg, echo=T, eval=F}
simulations %>%
  ggplot(data = .)+
  aes(x = stat)+
  geom_histogram(color="white", fill="indianred")+
  geom_vline(xintercept = sample_slope,
             color = "blue",
             size = 2,
             linetype = "dashed")+
  labs(x = expression(paste("Distribution of ", hat(beta[1]), " under ", H[0], " that ", beta[1]==0)),
       y = "Samples")+
    theme_classic(base_family = "Fira Sans Condensed",
           base_size=20)
```
]

--

.pull-right[
```{r, ref.label="visualize-gg", fig.height=5, fig.retina=3, fig.align="center"}

```
]

---

# What R Does: Classical Statistical Inference I

.pull-left[

- R does things the old-fashioned way, using a *theoretical* null distribution instead of *simulation*

- A .shout[`\\(t\\)`-distribution] with $n-k-1$ df<sup>1</sup>

- Calculate a $t$-statistic for $\hat{\beta_1}$:

$$\text{test statistic} = \frac{\text{estimate} - \text{null hypothesis}}{\text{standard error of estimate}}$$

]

.pull-right[

```{r, fig.retina=3, fig.align="center"}
t_ex<-ggplot(data = tibble(x=-5:5))+
  aes(x = x)+
  stat_function(fun = dt, args=list(df = 418), size=2, color="blue")+
  #stat_function(fun=dt, geom="area", fill="blue", xlim=c(-5,4.75), alpha=0.5)+
  #stat_function(fun=dt, geom="area", fill="blue", xlim=c(4.75,5), alpha=0.5)+
  geom_vline(xintercept = c(-4.75, 4.75), size = 1, color = "red", linetype = "dashed")+
  geom_rect(xmin=-5,xmax=-4.75, ymin=0, ymax=0.4, fill="red", alpha=0.3)+
  geom_rect(xmin=4.75,xmax=5, ymin=0, ymax=0.4, fill="red", alpha=0.3)+
  labs(x = "t",
       y = "Probability")+
  scale_x_continuous(breaks = seq(-5,5,1),
                     lim = c(-5,5),
                     expand = c(0,0))+
  scale_y_continuous(breaks = seq(0,0.4,0.1),
                     lim = c(0,0.4),
                     expand = c(0,0))+
  theme_classic(base_family = "Fira Sans Condensed",
           base_size=20)
t_ex
```

]

.footnote[<sup>1</sup> `\\(k\\)` is the number of `\\(X\\)` variables.]

---

# What R Does: Classical Statistical Inference II

.pull-left[

$$\text{test statistic} = \frac{\text{estimate} - \text{null hypothesis}}{\text{standard error of estimate}}$$

- $t$ has the same interpretation as $Z$, number of std. dev. away from the distribution's center<sup>1</sup>

- Compares to a critical value of $t^*$ (determined by $\alpha$ & $n-k-1$)
    - For 95% confidence, $\alpha=0.05$, $t^* \approx 2$<sup><span class="red">2</span></sup>
]

.pull-right[

```{r, fig.retina=3, fig.align="center"}
t_ex
```

]

.footnote[<sup>1</sup> Think of our simulated distribution, the center was 0.

<sup>2</sup> The 68-95-99.7% empirical rule!]

---

# What R Does: Classical Statistical Inference III

.pull-left[

$$\begin{align*}
t &= \frac{\hat{\beta_1}-\beta_{1,0}}{se(\hat{\beta_1})}\\
t &= \frac{-2.28-0}{0.48}\\
t &= -4.75\\ \end{align*}$$

- Our sample slope is 4.75 standard deviations below the mean under $H_0$

- $p$-value: prob. of a test statistic at least as large (in magnitude) as ours if the null hypothesis were true<sup>1</sup>
  - $p$-value is **2-sided** for $H_a: \beta_1 \neq 0$
]

.pull-right[

```{r, fig.retina=3, fig.align="center"}
t_ex
```

]

.footnote[<sup>1</sup> Think of our simulated distribution, the center was 0.]

---

# 1-Sided vs. 2-Sided p-values I

.pull-left[

$H_a: \beta_1<0$

$p$-value: $Prob(t < t_i)$

```{r, fig.retina=3, fig.height=5, fig.align="center"}
ggplot(data = tibble(x=-4:4))+
  aes(x = x)+
  stat_function(fun = dnorm, size=2, color="blue")+
  stat_function(fun=dnorm, xlim=c(-4,-2), geom="area", fill="blue", alpha=0.5)+
  labs(x = "t",
       y = "Probability")+
  scale_x_continuous(breaks=c(-2),
                      label=c(expression(-t[i])))+
  theme_classic(base_family = "Fira Sans Condensed",
           base_size=20)

```

]
.pull-right[

$H_a: \beta_1>0$

$p$-value: $Prob(t > t_i)$

```{r, fig.retina=3, fig.height=5, fig.align="center"}
ggplot(data = tibble(x=-4:4))+
  aes(x = x)+
  stat_function(fun = dnorm, size=2, color="blue")+
  stat_function(fun=dnorm, xlim=c(2,4), geom="area", fill="blue", alpha=0.5)+
  labs(x = "t",
       y = "Probability")+
  scale_x_continuous(breaks=c(2),
                      label=c(expression(t[i])))+
  theme_classic(base_family = "Fira Sans Condensed",
           base_size=20)
```


]

---

# 1-Sided vs. 2-Sided p-values I


$H_a: \beta_1 \neq 0$

$p$-value: $2 \times Prob(t > |t_i|)$

```{r, fig.retina=3, fig.height=5, fig.align="center"}
ggplot(data = tibble(x=-4:4))+
  aes(x = x)+
  stat_function(fun = dnorm, size=2, color="blue")+
  stat_function(fun=dnorm, xlim=c(-4,-2), geom="area", fill="blue", alpha=0.5)+
  stat_function(fun=dnorm, xlim=c(2,4), geom="area", fill="blue", alpha=0.5)+
  labs(x = expression(paste("Test Statistic t, (deviation from ", H[0], " if ", H[0], " were True)")),
       y = "Probability")+
  scale_x_continuous(breaks=c(-2,2),
                      label=c(expression(-t[i]), expression(t[i])))+
  theme_classic(base_family = "Fira Sans Condensed",
           base_size=20)
```

---

# Hypothesis Tests in Regression Output I

```{r, echo = T}
summary(school_reg)
```

---

# Hypothesis Tests in Regression Output II

- In `broom`'s `tidy()` (with confidence intervals)

```{r, echo = T}
tidy(school_reg, conf.int=TRUE)
```

---

# Conclusions

$$\begin{align*}
H_0: \,& \beta_1=0\\
H_a: \, & \beta_a \neq 0\\
\end{align*}$$

- Because the hypothesis test's $p$-value $<$ $\alpha$ (0.05)...

- .onfire[We have sufficient evidence to reject `\\(H_0\\)` in favor of our alternative hypothesis. Our sample suggests that there *is a relationship* between class size and test scores].

--

- Using the confidence intervals:

- .onfire[We are 95% confident that the true marginal effect of class size on test scores is between `\\(-3.22\\)` and `\\(-1.34\\)`.]

---

# Hypothesis Testing and Confidence Intervals: Relationship

- Confidence intervals are all *two-sided* by nature
$$CI_{0.95}=\left(\left[\hat{\beta_1}-2 \times se(\hat{\beta_1})\right], \, \left[\hat{\beta_1}+2 \times se(\hat{\beta_1}\right]) \right)$$

- Hypothesis test $(t$-test) of $H_0: \, \beta_1=0$ computes a $t$-value of<sup>.red[1]</sup>
$$t=\frac{\hat{\beta_1}}{se(\hat{\beta_1})}$$
and $p$<0.05 when $t\geq2$

--

- .whisper[If a confidence interval contains the `\\(H_0\\)` value (i.e. `\\(0\\)`, for our test), then *we fail to reject* `\\(H_0\\)`.]

.footnote[<sup>.red[1]</sup> Since our null hypothesis is that $\beta_{1,0}=0$, the test statistic simplifies to this neat fraction.]

---

# Common Misconceptions about p-values

- All of the following are **FALSE** interpretations of $p$ (and below are reasons why each is wrong)

1. $p$ **is the probability that the alternative hypothesis is false**
  - We can never *prove* an alternative hypothesis, only tentatively reject a null hypothesis

--

2. $p$ **is the probability that the null hypothesis is true**
  - We're not *proving* the $H_0$ is false, only saying that it's very unlikely that if $H_0$ were true, we'd obtain a slope as rare as our sample's slope

--

3. $p$ **is the probability that our observed effects were produced purely by random chance**
  - $p$ is computed under a specific model (think about our null world) that *assumes* $H_0$ is true

--

4. $p$ **tells us how significant our finding is**
  - $p$ tells us nothing about the *size* or the *real world significance* of any effect deemed "statistically significant"
  - it only tells us that the slope is statistically significantly different from 0 (if $H_0$ is $\beta_1=0)$ 

---

# Abusing p-Values I

.pull-left[
.center[
![:scale 60%](https://www.dropbox.com/s/9nv7ewnkokuek3c/smbc1623-1.png?raw=1)
]
]

--

.pull-right[
.center[
![:scale 60%](https://www.dropbox.com/s/5xdh79z3uwg5n3b/smbc1623-2.png?raw=1)
]
]

.source[Source: [SMBC](http://www.smbc-comics.com/?id=1623)]

---

# Abusing p-Values II

.pull-left[

.center[
![](https://www.dropbox.com/s/70gxtqm7tmag454/asalogo.jpg?raw=1)
]
]

.pull-right[

>"The widespread use of 'statistical significance' (generally interpreted as $(p \leq 0.05)$ as a license for making a claim of a scientific finding (or implied truth) leads to considerable distortion of the scientific process."

]

.source[Wasserstein, Ronald L. and Nicole A. Lazar, (2016), ["The ASA's Statement on p-Values: Context, Process, and Purpose](http://www.tandfonline.com/doi/full/10.1080/00031305.2016.1154108)," *The American Statistician* 30(2): 129-133]

---

# p-value Clarification

- Again, .shout[p-value is the probability that, assuming the null hypothesis is true, we obtain (by pure random chance) a test statistic at least as extreme as the one we estimated for our sample]

- A low p-value means either (and we can't distinguish which):
    1. $H_0$ is true and a highly improbable event has occurred OR
    2. $H_0$ is false

---

# Significance In Regression Tables

.pull-left[
.font80[
.regtable[
```{r}
library(huxtable)
huxreg("Test Score" = school_reg,
       coefs = c("Intercept" = "(Intercept)",
                 "STR" = "str"),
       statistics = c("N" = "nobs",
                      "R-Squared" = "r.squared",
                      "SER" = "sigma"),
       number_format = 2)
```
]
]
]

.pull-right[

- Statistical significance is shown by asterisks, common (but not always!) standard:
    - 1 asterisk: significant at $\alpha=0.10$
    - 2 asterisks: significant at $\alpha=0.05$
    - 3 asterisks: significant at $\alpha=0.01$

- Rare, but sometimes regression tables include $p$-values for estimates
]