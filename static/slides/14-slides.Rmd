---
title: "3.2: Multivariate OLS Estimators: Bias, Precision, and Fit"
subtitle: "ECON 480 · Econometrics · Fall 2019"
author: 'Ryan Safner<br> Assistant Professor of Economics <br> <a href="mailto:safner@hood.edu"><i class="fa fa-paper-plane fa-fw"></i> safner@hood.edu</a> <br> <a href="https://github.com/ryansafner/metricsf19"><i class="fa fa-github fa-fw"></i> ryansafner/metricsf19</a><br> <a href="https://metricsF19.classes.ryansafner.com"> <i class="fa fa-globe fa-fw"></i> metricsF19.classes.ryansafner.com</a><br>'
#date:
output:
  xaringan::moon_reader:
    mathjax: "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_HTMLorMML" # rescales math with css changes https://github.com/yihui/xaringan/issues/143
    lib_dir: libs
    df_print: paged
    #seal: false
    css: [custom.css, custom-fonts.css, "hygge"] #, metropolis, metropolis-fonts
    nature:
      beforeInit: ["macros.js", "https://platform.twitter.com/widgets.js"] # first is for rescaling images , second is for embedding tweets, https://github.com/yihui/xaringan/issues/100
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ratio: "16:9"
      slideNumberFormat: |
        <div class="progress-bar-container">
          <div class="progress-bar" style="width: calc(%current% / %total% * 100%);">
          </div>
        </div>
    includes:
      in_header: header.html # for font awesome, used in title  
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(echo=F,
                      message=F,
                      warning=F)
library(tidyverse)
set.seed(256)
update_geom_defaults("label", list(family = "Fira Sans Condensed"))
```

```{r regression-setup, echo=F, results="hide"}

library(haven)
CASchool<-read_dta("../data/caschool.dta")

# run regression of testscr on str
school_reg <- lm(testscr ~ str, 
                 data = CASchool)

library(broom)
school_reg_tidy <- tidy(school_reg,
     conf.int = TRUE) # add confidence intervals
CASchool_aug <- augment(school_reg)

library(equatiomatic)
extract_eq(school_reg, use_coefs = TRUE,coef_digits = 2,fix_signs = TRUE)

```

class: inverse, center, middle

# The Multivariate OLS Estimators

---

# The Multivariate OLS Estimators

- By analogy, we still focus on the .shout[ordinary least squares (OLS) estimators] of the unknown population parameters $\beta_0, \beta_1, \beta_2, \cdots, \beta_k$ which solves:

$$\min_{\hat{\beta_0}, \hat{\beta_1}, \hat{\beta_2}, \cdots, \hat{\beta_k}} \sum^n_{i=1}\left[\underbrace{Y_i-(\hat{\beta_0}+\hat{\beta_1}X_{1i}+\hat{\beta_2}X_{2i}+\cdots+ \hat{\beta_k}X_{ki})}_{u_i}\right]^2$$

- Again, OLS estimators are chosen to **minimize** the **sum of squared errors (SSE)**
  - i.e. the sum of the squared distances between the actual values of $Y_i$ and the predicted values $\hat{Y_i}$

---

# The Multivariate OLS Estimators: FYI

.content-box-red[
.red[**Math FYI]**: in linear algebra terms, a regression model with $n$ observations of $k$ independent variables:

$$\mathbf{Y} = \mathbf{X \beta}+\mathbf{u}$$

$$\underbrace{\begin{pmatrix}
			y_1\\
			y_2\\
			\vdots \\
			y_n\\
		\end{pmatrix}}_{\mathbf{Y}_{(n \times 1)}}
		=
		\underbrace{\begin{pmatrix}
	x_{1,1} & x_{1,2} & \cdots & x_{1,n}\\
	x_{2,1} & x_{2,2} & \cdots & x_{2,n}\\
	\vdots & \vdots & \ddots & \vdots\\
	x_{k,1} & x_{k,2} & \cdots & x_{k,n}\\ 
\end{pmatrix}}_{\mathbf{X}_{(n \times k)}}
		\underbrace{\begin{pmatrix}
\beta_1\\
\beta_2\\
\vdots \\
\beta_k \\	
\end{pmatrix}}_{\mathbf{\beta}_{(k \times 1)}}
+
		\underbrace{\begin{pmatrix}
			u_1\\
			u_2\\
			\vdots \\
			u_n \\
		\end{pmatrix}}_{\mathbf{u}_{(n \times 1)}}$$

]

--

- The OLS estimator for $\beta$ is $\hat{\beta}=(\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'\mathbf{Y}$

--

- Appreciate that I am saving you from such sorrow

---

class: center, middle, inverse

# The Sampling Distribution of $\hat{\beta_j}$

---

# The Sampling Distribution of $\hat{\beta_j}$ I

.pull-left[

- For any individual $\beta_j$, it has a sampling distribution: 

$$\hat{\beta_j} \sim N \left(E[\hat{\beta_j}], \;se(\hat{\beta_j})\right)$$

- We want to know its sampling distribution's:
  - **Center**: $E[\hat{\beta_j}]$; what is the *expected value* of our estimator?
  - **Spread**: $se(\hat{\beta_j})$; how *precise* is our estimator?
]

.pull-right[

```{r, fig.retina=3}
beta_dist<-ggplot(data = tibble(x=-4:4))+
  aes(x = x)+
  stat_function(fun = dnorm, size=2, color="blue")+
  geom_segment(aes(x=0,xend=0, y=0, yend=0.4), linetype="dashed")+
  scale_x_continuous(breaks = 0,
                     labels = expression(E(hat(beta[j]))))+
  labs(x = expression(hat(beta[j])),
       y = "Probability")+
  theme_classic(base_family = "Fira Sans Condensed",
           base_size=20)
beta_dist
```
]

---

class: inverse, center, middle

# The Expected Value of $\hat{\beta_j}$: Bias

---

# Exogeneity and Unbiasedness

- As before, $E[\hat{\beta_j}]=\beta_j$ when $X_j$ is .onfire[exogenous] (i.e. $cor(X_j, u)=0$)

- We know the true $E[\hat{\beta_j}]=\beta_j+\underbrace{cor(X_j,u)\frac{\sigma_u}{\sigma_{X_j}}}_{\text{O.V. Bias}}$

- If $X_j$ is .shout[endogenous] (i.e. $cor(X_j, u)\neq 0$), contains **omitted variable bias**

- We can now try to *quantify* the omitted variable bias

---

# Measuring Omitted Variable Bias I

- Suppose the **_true_ population model** of a relationship is:

$$Y_i=\beta_0+\beta_1 X_{1i}+\beta_2 X_{2i}+u_i$$

- What happens when we run a regression and **omit** $X_{2i}$?

--

- Suppose we estimate the following .onfire[omitted regression] of just $Y_i$ on $X_{1i}$ (omitting $X_{2i})$:<sup>.red[+]</sup>

$$Y_i=\alpha_0+\alpha_1 X_{1i}+\nu_i$$

.footnote[<sup>.red[+]</sup> Note: I am using `\\(\alpha\\)`'s and `\\(\nu_i\\)` only to denote these are different estimates than the **true** model `\\(\beta\\)`'s and `\\(u_i\\)`]

---

# Measuring Omitted Variable Bias II

- .onfire[**Key Question**:] are $X_{1i}$ and $X_{2i}$ correlated?

--

- Run an .onfire[auxiliary regression] of $X_{2i}$ on $X_{1i}$ to see:<sup>.red[+]</sup>

$$X_{2i}=\delta_0+\delta_1 X_{1i}+\tau_i$$

--

- If $\delta_1=0$, then $X_{1i}$ and $X_{2i}$ are *not* linearly related

- If $|\delta_1|$ is very big, then $X_{1i}$ and $X_{2i}$ are strongly linearly related

.footnote[<sup>.red[+]</sup> Note: I am using `\\(\delta\\)`'s and `\\(\tau\\)` to differentiate estimates for this model.]

---

# Measuring Omitted Variable Bias III

- Now substitute our .onfire[auxiliary regression] between $X_{2i}$ and $X_{1i}$ into the .shout[*true* model]:
  - We know $X_{2i}=\delta_0+\delta_1 X_{1i}+\tau_i$

$$\begin{align*}
	Y_i&=\beta_0+\beta_1 X_{1i}+\beta_2 X_{2i}+u_i	\\
\end{align*}$$

---

# Measuring Omitted Variable Bias III

- Now substitute our .onfire[auxiliary regression] between $X_{2i}$ and $X_{1i}$ into the .shout[*true* model]:
  - We know $X_{2i}=\delta_0+\delta_1 X_{1i}+\tau_i$

$$\begin{align*}
	Y_i&=\beta_0+\beta_1 X_{1i}+\beta_2 X_{2i}+u_i	\\
	Y_i&=\beta_0+\beta_1 X_{1i}+\beta_2 \big(\delta_0+\delta_1 X_{1i}+\tau_i \big)+u_i	\\
\end{align*}$$

---
# Measuring Omitted Variable Bias III

- Now substitute our .onfire[auxiliary regression] between $X_{2i}$ and $X_{1i}$ into the .shout[*true* model]:
  - We know $X_{2i}=\delta_0+\delta_1 X_{1i}+\tau_i$

$$\begin{align*}
	Y_i&=\beta_0+\beta_1 X_{1i}+\beta_2 X_{2i}+u_i	\\
	Y_i&=\beta_0+\beta_1 X_{1i}+\beta_2 \big(\delta_0+\delta_1 X_{1i}+\tau_i \big)+u_i	\\
Y_i&=(\beta_0+\beta_2 \delta_0)+(\beta_1+\beta_2 \delta_1)X_{1i}+(\beta_2 \tau_i+u_i)\\
\end{align*}$$

---

# Measuring Omitted Variable Bias III

- Now substitute our .onfire[auxiliary regression] between $X_{2i}$ and $X_{1i}$ into the .shout[*true* model]:
  - We know $X_{2i}=\delta_0+\delta_1 X_{1i}+\tau_i$

$$\begin{align*}
	Y_i&=\beta_0+\beta_1 X_{1i}+\beta_2 X_{2i}+u_i	\\
	Y_i&=\beta_0+\beta_1 X_{1i}+\beta_2 \big(\delta_0+\delta_1 X_{1i}+\tau_i \big)+u_i	\\
Y_i&=(\underbrace{\beta_0+\beta_2 \delta_0}_{\alpha_0})+(\underbrace{\beta_1+\beta_2 \delta_1}_{\alpha_1})X_{1i}+(\underbrace{\beta_2 \tau_i+u_i}_{\nu_i})\\
\end{align*}$$

- Now relabel each of the three terms as the OLS estimates $(\alpha$'s) and error $(\nu_i)$ from the .onfire[omitted regression], so we again have:

$$Y_i=\alpha_0+\alpha_1X_{1i}+\nu_i$$

--

- Crucially, this means that our OLS estimate for $X_{1i}$ in the .onfire[omitted regression] is:
$$\alpha_1=\beta_1+\beta_2 \delta_1$$

---

# Measuring Omitted Variable Bias IV

.center[
$\alpha_1=$.green[`\\(\beta_1\\)`] $+$ .red[`\\(\beta_2\\)`].purple[`\\(\delta_1\\)`]
]

- The **Omitted Regression** OLS estimate for $X_{1i}$ picks up *both*:

1. .green[The true effect of `\\(X_{1}\\)` on `\\(Y_i\\)`: `\\((\beta_1)\\)`]

2. .red[The true effect of `\\(X_{2}\\)` on `\\(Y_i\\)`: `\\((\beta_2)\\)`]
  - As pulled through .purple[the relationship between `\\(X_1\\)` and `\\(X_2\\)`: `\\((\delta_1)\\)`]

--

- Recall our conditions for omitted variable bias from some variable $Z_i$:

1. $\mathbf{Z_i}$ **must be a determinant of $Y_i$** $\implies$ .red[`\\(\beta_2 \neq 0\\)`]

2. $\mathbf{Z_i}$ **must be correlated with $X_i$** $\implies$ .purple[`\\(\delta_1 \neq 0\\)`]

--

- Otherwise, if $Z_i$ does not fit these conditions, $\alpha_1=\beta_1$ and the .onfire[omitted regression] is *unbiased*!

---

# Measuring Omitted Variable Bias in Our Class Size Example I

.pull-left[

```{r}
true<-lm(testscr~str+el_pct, data=CASchool)
summary(true)
```
]

.pull-right[

- The "True" Regression $(Y_i$ on $X_{1i}$ and $X_{2i})$

$$\widehat{\text{Test Score}_i}=686.03-1.10\text{ STR}_i-0.65\text{ %EL}_i$$
]

---

# Measuring Omitted Variable Bias in Our Class Size Example II

.pull-left[

```{r}
omitted<-lm(testscr~str, data=CASchool)
summary(omitted)
```
]

.pull-right[

- The "Omitted" Regression $(Y_{i}$ on just $X_{1i})$

$$\widehat{\text{Test Score}_i}=698.93-2.28\text{ STR}_i$$
]

---

# Measuring Omitted Variable Bias in Our Class Size Example III

.pull-left[

```{r}
aux<-lm(el_pct~str, data=CASchool)
summary(aux)
```
]

.pull-right[

- The "Auxiliary" Regression $(X_{2i}$ on $X_{1i})$

$$\widehat{\text{%EL}_i}=-19.85+1.81\text{ STR}_i$$
]

---

# Measuring Omitted Variable Bias in Our Class Size Example IV

.pull-left[
.center[
"True" Regression

$$\widehat{\text{Test Score}_i}=686.03-1.10\text{ STR}_i-0.65\text{ %EL}$$

"Omitted" Regression

$\widehat{\text{Test Score}_i}=698.93$ .blue[`\\(-2.28\\)`] $\text{ STR}_i$

"Auxiliary" Regression

$$\widehat{\text{%EL}_i}=-19.85+1.81\text{ STR}_i$$

]
]

.pull-right[

- Omitted Regression estimate for $\alpha_1$ on STR is .blue[`\\(-2.28\\)`]
]

---

# Measuring Omitted Variable Bias in Our Class Size Example IV

.pull-left[
.center[
"True" Regression

$\widehat{\text{Test Score}_i}=686.03$ .green[`\\(-1.10\\)`] $\text{ STR}_i-0.65\text{ %EL}$

"Omitted" Regression

$\widehat{\text{Test Score}_i}=698.93$ .blue[`\\(-2.28\\)`] $\text{ STR}_i$

"Auxiliary" Regression

$\widehat{\text{%EL}_i}=-19.85+1.81\text{ STR}_i$

]
]

.pull-right[

- Omitted Regression estimate for $\alpha_1$ on STR is .blue[`\\(-2.28\\)`]

.center[
$\alpha_1=$.green[`\\(\beta_1\\)`] $+$ .red[`\\(\beta_2\\)`].purple[`\\(\delta_1\\)`]
]

- .green[The true effect of STR on Test Score: -1.10]

]

---

# Measuring Omitted Variable Bias in Our Class Size Example IV

.pull-left[
.center[
"True" Regression

$\widehat{\text{Test Score}_i}=686.03$ .green[`\\(-1.10\\)`] $\text{ STR}_i$ .red[`\\(-0.65\\)`] $\text{ %EL}$

"Omitted" Regression

$\widehat{\text{Test Score}_i}=698.93$ .blue[`\\(-2.28\\)`] $\text{ STR}_i$

"Auxiliary" Regression

$\widehat{\text{%EL}_i}=-19.85+1.81\text{ STR}_i$

]
]

.pull-right[

- Omitted Regression estimate for $\alpha_1$ on STR is .blue[`\\(-2.28\\)`]

.center[
$\alpha_1=$.green[`\\(\beta_1\\)`] $+$ .red[`\\(\beta_2\\)`].purple[`\\(\delta_1\\)`]
]

- .green[The true effect of STR on Test Score: -1.10]

- .red[The true effect of %EL on Test Score: -0.65]

]

---

# Measuring Omitted Variable Bias in Our Class Size Example IV

.pull-left[
.center[
"True" Regression

$\widehat{\text{Test Score}_i}=686.03$ .green[`\\(-1.10\\)`] $\text{ STR}_i$ .red[`\\(-0.65\\)`] $\text{ %EL}$

"Omitted" Regression

$\widehat{\text{Test Score}_i}=698.93$ .blue[`\\(-2.28\\)`] $\text{ STR}_i$

"Auxiliary" Regression

$\widehat{\text{%EL}_i}=-19.85+$ .purple[1.81] $\text{ STR}_i$

]
]

.pull-right[

- Omitted Regression estimate for $\alpha_1$ on STR is .blue[`\\(-2.28\\)`]

.center[
$\alpha_1=$.green[`\\(\beta_1\\)`] $+$ .red[`\\(\beta_2\\)`].purple[`\\(\delta_1\\)`]
]

- .green[The true effect of STR on Test Score: -1.10]

- .red[The true effect of %EL on Test Score: -0.65]

- .purple[The relationship between STR and %EL: 1.81]
]

---

# Measuring Omitted Variable Bias in Our Class Size Example IV

.pull-left[
.center[
"True" Regression

$\widehat{\text{Test Score}_i}=686.03$ .green[`\\(-1.10\\)`] $\text{ STR}_i$ .red[`\\(-0.65\\)`] $\text{ %EL}$

"Omitted" Regression

$\widehat{\text{Test Score}_i}=698.93$ .blue[`\\(-2.28\\)`] $\text{ STR}_i$

"Auxiliary" Regression

$\widehat{\text{%EL}_i}=-19.85+$ .purple[1.81] $\text{ STR}_i$

]
]

.pull-right[

- Omitted Regression estimate for $\alpha_1$ on STR is .blue[`\\(-2.28\\)`]

.center[
$\alpha_1=$.green[`\\(\beta_1\\)`] $+$ .red[`\\(\beta_2\\)`].purple[`\\(\delta_1\\)`]
]

- .green[The true effect of STR on Test Score: -1.10]

- .red[The true effect of %EL on Test Score: -0.65]

- .purple[The relationship between STR and %EL: 1.81]

- So, for the .onfire[omitted regression]:

.center[
.blue[`\\(\alpha_1 =-2.28\\)`] $=$.green[`\\(-1.10\\)`] $+$ .red[`\\(-0.65\\)`] $($ .purple[`\\(1.81\\)`] $)$

]


]

---

# Measuring Omitted Variable Bias in Our Class Size Example IV

.pull-left[
.center[
"True" Regression

$\widehat{\text{Test Score}_i}=686.03$ .green[`\\(-1.10\\)`] $\text{ STR}_i$ .red[`\\(-0.65\\)`] $\text{ %EL}$

"Omitted" Regression

$\widehat{\text{Test Score}_i}=698.93$ .blue[`\\(-2.28\\)`] $\text{ STR}_i$

"Auxiliary" Regression

$\widehat{\text{%EL}_i}=-19.85+$ .purple[1.81] $\text{ STR}_i$

]
]

.pull-right[

- Omitted Regression estimate for $\alpha_1$ on STR is .blue[`\\(-2.28\\)`]

.center[
$\alpha_1=$.green[`\\(\beta_1\\)`] $+$ .red[`\\(\beta_2\\)`].purple[`\\(\delta_1\\)`]
]

- .green[The true effect of STR on Test Score: -1.10]

- .red[The true effect of %EL on Test Score: -0.65]

- .purple[The relationship between STR and %EL: 1.81]

- So, for the .onfire[omitted regression]:

.center[
.blue[`\\(\alpha_1 =-2.28\\)`] $=$.green[`\\(-1.10\\)`] $+$ .red[`\\(-0.65\\)`] $($ .purple[`\\(1.81\\)`] $)$



- The **bias** is $($ .red[`\\(-0.65\\)`] $)($ .purple[`\\(1.81\\)`] $)=\mathbf{-1.18}$

]
]

---
class: inverse, center, middle
# Precision of $\hat{\beta_j}$

---

# Precision of $\hat{\beta_j}$ I

.pull-left[

- $\sigma_{\hat{\beta_j}}$; how **precise** are our estimates? (today)
    - <span class="shout">Variance $\sigma^2_{\hat{\beta_j}}$</span> or <span class="shout">standard error $\sigma_{\hat{\beta_j}}$</span>
]

.pull-right[

```{r, fig.retina=3}
beta_dist+
  geom_label(x=1, y=dnorm(1), label=expression(sigma[hat(beta[j])]==1), color="blue")+
  stat_function(fun = dnorm, args=list(mean = 0, sd = 2), size=2, color="red")+
  geom_label(x=2, y=dnorm(2,0,2), label=expression(sigma[hat(beta[j])]==2), color="red")
```
]

---

# Precision of $\hat{\beta_j}$ II

.pull-left[

$$var(\hat{\beta_j})=\underbrace{\frac{1}{1-R^2_j}}_{VIF} \times \frac{(SER)^2}{n \times var(X)}$$

$$se(\hat{\beta_j})=\sqrt{var(\hat{\beta_1})}$$

]

.pull-right[

- Variation in $\hat{\beta_j}$ is affected by **four** things now:

1. **Goodness of fit of the model $(SER)$**
    - Larger $SER$ $\rightarrow$ larger $var(\hat{\beta_j})$
2. **Sample size, $n$**
    - Larger $n$ $\rightarrow$ smaller $var(\hat{\beta_j})$
3. **Variance of $X$**
    - Larger $var(X)$ $\rightarrow$ smaller $var(\hat{\beta_j})$
4. **Variance Inflation Factor** $\frac{1}{(1-R^2_j)}$
    - Larger $VIF$, larger $var(\hat{\beta_j})$
    - This is the only new effect, explained in a moment

]

.footnote[<sup>.red[+]</sup> See Class 2.5 for a reminder of variation with just variable.]

---

# VIF and Multicollinearity I

- Two *independent* variables are .shout[multicollinear]:

$$cor(X_j, X_l) \neq 0 \quad \forall j \neq l$$

--

- .onfire[Multicollinearity between X variables does *not bias* OLS estimates]
  - Remember, we pulled another variable out of $u$ into the regression
  - If it were omitted, then it *would* cause omitted variable bias! 

--

- .onfire[Multicollinearity does *increase the variance* of an estimate] by

$$VIF=\frac{1}{(1-R^2_j)}$$

---

# VIF and Multicollinearity II

$$VIF=\frac{1}{(1-R^2_j)}$$

- $R^2_j$ is the $R^2$ from an **auxiliary regression** of $X_j$ on all other regressors $(X$'s)

--

.content-box-green[
.green[**Example**]: Suppose we have a regression with three regressors $(k=3)$:

$$Y_i=\beta_0+\beta_1X_{1i}+\beta_2X_{2i}+\beta_3X_{3i}$$
]

--

- There will be three different $R^2_j$'s, one for each regressor:

$$\begin{align*}
R^2_1 \text{ for }  X_{1i}&=\gamma+\gamma X_{2i} + \gamma X_{3i}	\\
R^2_2 \text{ for }  X_{2i}&=\zeta_0+\zeta_1 X_{1i} + \zeta_2 X_{3i}	\\
R^2_3 \text{ for }  X_{3i}&=\eta_0+\eta_1 X_{1i} + \eta_2 X_{2i}	\\
  \end{align*}$$

---

# VIF and Multicollinearity III

$$VIF=\frac{1}{(1-R^2_j)}$$

- $R^2_j$ is the $R^2$ from an **auxiliary regression** of $X_j$ on all other regressors $(X$'s)

- The $R_j^2$ tells us .onfire[how much *other* regressors explain regressor `\\(X_j\\)`]

- **Key Takeaway**: If other $X$ variables explain $X_j$ well (high $R^2_J$), it will be harder to tell how *cleanly* $X_j \rightarrow Y_i$, and so $var(\hat{\beta_j})$ will be higher

---

# VIF and Multicollinearity IV

- Common to calculate the .shout[Variance Inflation Factor (VIF)] for each regressor:

$$VIF=\frac{1}{(1-R^2_j)}$$

- VIF quantifies the factor (scalar) by which $var(\hat{\beta_j})$ increases because of multicollinearity
  - e.g. increases 2x, 3x, etc.
--

- Baseline: $R^2_j=0$ $\implies$ *no* multicollinearity $\implies$ VIF = 1 (no inflation)

--

- Larger $R^2_j$ $\implies$ larger VIF
    - Rule of thumb: $VIF>10$ is worrisome 

---

# VIF and Multicollinearity V

.pull-left[
```{r el-str-scatter, echo=T, eval=F}
ggplot(data=CASchool, aes(x=str,y=el_pct))+
  geom_point(color="blue")+
  geom_smooth(method="lm", color="red")+
  labs(x = "Student to Teacher Ratio",
       y = "Percentage of ESL Students")+
    theme_classic(base_family = "Fira Sans Condensed",
           base_size=20)
```

```{r, echo=T}
CAcorr_ex<-subset(CASchool, select=c("testscr", "str", "el_pct"))

# Make a correlation table
cor(CAcorr_ex)
```

]

.pull-right[
```{r ref.label="el-str-scatter", fig.retina=3}

```

]

---

# VIF and Multicollinearity in R I

```{r, echo=T}
# our multivariate regression
elreg<-lm(testscr~str+el_pct,data=CASchool)

# use the "car" package for VIF function 
library("car") 

# syntax: vif(lm.object)
vif(elreg)
```

--

- $var(\hat{\beta_1})$ on `str` increases by 1.036 times due to multicollinearity with `el_pct`
- $var(\hat{\beta_2})$ on `el_pct` increases by 1.036 times due to multicollinearity with `str`

---

# VIF and Multicollinearity in R II

- Let's calculate VIF manually to see where it comes from:

--

```{r,echo=T}
# run auxiliary regression of x2 on x1

auxreg<-lm(el_pct~str, data=CASchool)
# use broom package's tidy() command (cleaner)
library(broom) # load broom
tidy(auxreg) # look at reg output
```

---

# VIF and Multicollinearity in R III

```{r,echo=T}
glance(auxreg) # look at aux reg stats for R^2

# extract our R-squared from aux regression (R_j^2)
aux_r_squared<-glance(auxreg) %>%
  pull(r.squared)
aux_r_squared # look at it
```

---

# VIF and Multicollinearity in R IV

```{r,echo=T}
# calculate VIF manually

our_vif<-1/(1-aux_r_squared) # VIF formula 
our_vif
```

- Again, multicollinearity between the two $X$ variables inflates the variance on each by 1.036 times

---

# VIF and Multicollinearity: Another Example I

.content-box-green[
.green[**Example**:] For our Test Scores and Class Size example, what about district expenditures per student?
]

```{r, echo=T}
CAcorr2<-subset(CASchool, select=c("testscr", "str", "expn_stu"))

# Make a correlation table
corr2<-cor(CAcorr2)

# look at it
corr2
```

---

# VIF and Multicollinearity: Another Example II

.pull-left[
```{r exp-str-scatter, echo=T, eval=F}
ggplot(data=CASchool, aes(x=str,y=expn_stu))+
  geom_point(color="blue")+
  geom_smooth(method="lm", color="red")+
  scale_y_continuous(labels = scales::dollar)+
  labs(x = "Student to Teacher Ratio",
       y = "Expenditures per Student ($)")+
    theme_classic(base_family = "Fira Sans Condensed",
           base_size=20)
```

]

.pull-right[
```{r ref.label="exp-str-scatter", fig.retina=3}

```

]

---

# VIF and Multicollinearity: Another Example III

1. $cor(\text{Test score, expn})\neq0$

2. $cor(\text{STR, expn})\neq 0$

--

- Omitting $expn$ will **bias** $\hat{\beta_1}$ on STR

--

- *Including* $expn$ will *not* bias $\hat{\beta_1}$ on STR, but *will* make it less precise (higher variance)

--

- Data tells us little about the effect of a change in $STR$ holding $expn$ constant
  - Hard to know what happens to test scores when high $STR$ AND high $expn$ and vice versa (*they rarely happen simultaneously*)!


---

# VIF and Multicollinearity: Another Example IV

.pull-left[

```{r, echo=T}
expreg<-lm(testscr~str+expn_stu, data=CASchool)
summary(expreg)
```

]

.pull-right[
```{r, echo=T}
vif(expreg)
```

- Including `expn_stu` increases variance of $\hat{\beta_1}$ by 1.62 times 

]

---

# Multicollinearity Increases Variance

.pull-left[
```{r multioutput, echo=T, eval=F}
library(huxtable)
huxreg("Model 1" = school_reg,
       "Model 2" = expreg,
       coefs = c("Intercept" = "(Intercept)",
                 "Class Size" = "str",
                 "Expenditures per Student" = "expn_stu"),
       statistics = c("N" = "nobs",
                      "R-Squared" = "r.squared",
                      "SER" = "sigma"),
       number_format = 2)
```

- We can see $SE(\hat{\beta_1})$ on `str` increases from 0.48 to 0.61 when we add `expn_stu` 

]

.pull-right[
```{r, ref.label="multioutput"}
```


]

---

# Perfect Multicollinearity

- .shout[*Perfect* multicollinearity] is when a regressor is an exact linear function of (an)other regressor(s)

--

$$\widehat{Sales} = \hat{\beta_0}+\hat{\beta_1}\text{Temperature (C)} + \hat{\beta_2}\text{Temperature (F)}$$

--

$$\text{Temperature (F)}=32+1.8*\text{Temperature (C)}$$

--

- $cor(\text{temperature (F), temperature (C)})=1$

--

- $R^2_j=1$ is implying $VIF=\frac{1}{1-1}$ and $var(\hat{\beta_j})=0$!

--

- **This is fatal for a regression**
  - A logical impossiblity, *always caused by human error*

---

# Perfect Multicollinearity: Example

.content-box-green[
.green[**Example**:]

$$\widehat{TestScore_i} = \hat{\beta_0}+\hat{\beta_1}STR_i	+\hat{\beta_2}\%EL+\hat{\beta_3}\%ES$$

]

- %EL: the percentage of students learning English

- %ES: the percentage of students fluent in English

- $ES=100-EL$

- $|cor(ES, EL)|=1$

---

# Perfect Multicollinearity Example II

```{r, echo=T}
# generate %EF variable from %EL
CASchool_ex <- CASchool %>%
  mutate(ef_pct = 100 - el_pct)

CASchool_ex %>%
  summarize(cor = cor(ef_pct, el_pct))
```

---

# Perfect Multicollinearity Example III

.pull-left[
```{r multicol, echo=T, eval=F}
ggplot(data=CASchool_ex, aes(x=el_pct,y=ef_pct))+
  geom_point(color="blue")+
  scale_y_continuous(labels = scales::dollar)+
  labs(x = "Percent of ESL Students",
       y = "Percent of Non-ESL Students")+
    theme_classic(base_family = "Fira Sans Condensed",
           base_size=20)
```

]

.pull-right[
```{r ref.label="multicol", fig.retina=3}

```

]

---

# Perfect Multicollinearity Example IV

.pull-left[
```{r, echo=T}
mcreg<-lm(testscr~str+el_pct+ef_pct, data=CASchool_ex)
summary(mcreg)
```

]

.pull-right[
- Note `R` *ignores* one of the multicollinear regressors (`ef_pct`) if you include both in a regression
]

---

class: inverse, center, middle

# A Summary of Multivariate OLS Estimator Properties

---

# A Summary of Multivariate OLS Estimator Properties

- $\hat{\beta_j}$ on $X_j$ is biased only if there is an omitted variable $(Z)$ such that: 
    1. $cor(Y,Z)\neq 0$
    2. $cor(X_j,Z)\neq 0$
    - If $Z$ is *included* and $X_j$ is collinear with $Z$, this does *not* cause a bias

- $var[\hat{\beta_j}]$ and $se[\hat{\beta_j}]$ measure precision of estimate:

--

$$var[\hat{\beta_j}]=\frac{1}{(1-R^2_j)}*\frac{SER^2}{n \times var[X_j]}$$

- VIF from multicollinearity: $\frac{1}{(1-R^2_j)}$
    - $R_j^2$ for auxiliary regression of $X_j$ on all other $X$'s
    - mutlicollinearity does not bias $\hat{\beta_j}$ but raises its variance 
    - *perfect* multicollinearity if $X$'s are linear function of others 

---

class: inverse, center, middle

# Updated Measures of Fit

---

# (Updated) Measures of Fit

- Again, how well does a linear model fit the data?

- How much variation in $Y_i$ is "explained" by variation in the model ($\hat{Y_i}$)?

--

$$\begin{align*}
Y_i&=\hat{Y_i}+\hat{u_i}\\
\hat{u_i}&= Y_i-\hat{Y_i}\\
\end{align*}$$

---

# (Updated) Measures of Fit: SER

- Again, the .shout[Standard errror of the regression (SER)] estimates the standard error of $u$ 

$$SER=\frac{SSE}{n-\mathbf{k}-1}$$

- A measure of the spread of the observations around the regression line (in units of $Y$), the average "size" of the residual

- **Only new change:** divided by $n-\mathbf{k}-1$ due to use of $k+1$ degrees of freedom to first estimate $\beta_0$ and then all of the other $\beta$'s for the $k$ number of regressors<sup>.red[1]</sup>

.footnote[<sup>.red[1]</sup> Again, because your textbook defines $k$ as including the constant, the denominator would be $n-k$ instead of $n-k-1$.]

---

# (Updated) Measures of Fit: $R^2$

$$\begin{align*}
R^2&=\frac{ESS}{TSS}\\
&=1-\frac{SSE}{TSS}\\
&=(r_{X,Y})^2 \\ \end{align*}$$

- Again, $R^2$ is fraction of variation of the model $(\hat{Y_i}$ ("explained SS") to the variation of observations of $Y_i$ ("total SS") 

---

# (Updated) Measures of Fit: Adjusted $\bar{R}^2$

- Problem: $R^2$ of a regression increases *every* time a new variable is added (it reduces SSE!)

- This does *not* mean adding a variable improves the fit of the model per se, $R^2$ gets **inflated**

--

-  We correct for this effect with the .shout[adjusted `\\(R^2\\)`]: 

$$\bar{R}^2 = 1 - \frac{n-1}{n-k-1} \times \frac{SSE}{TSS}$$

- There are different methods to compute $\bar{R}^2$, and in the end, recall $R^2$ **was never very useful**, so don't worry about knowing the formula
  - Large sample sizes $(n)$ make $R^2$ and $\bar{R}^2$ very close

---

# In R (base)

.pull-left[
```{r, echo=F}
summary(elreg)
```
]

.pull-right[
- Base $R^2$ (`R` calls it `multiple R-squared`) went up 
- `Adjusted R-squared` went down 
]

---

# In R (broom)

.pull-left[
```{r, echo=T}
elreg %>%
  glance()
```
]
