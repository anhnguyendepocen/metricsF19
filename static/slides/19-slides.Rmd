---
title: "3.7: Regression with Interaction Effects"
subtitle: "ECON 480 · Econometrics · Fall 2019"
author: 'Ryan Safner<br> Assistant Professor of Economics <br> <a href="mailto:safner@hood.edu"><i class="fa fa-paper-plane fa-fw"></i> safner@hood.edu</a> <br> <a href="https://github.com/ryansafner/metricsf19"><i class="fa fa-github fa-fw"></i> ryansafner/metricsf19</a><br> <a href="https://metricsF19.classes.ryansafner.com"> <i class="fa fa-globe fa-fw"></i> metricsF19.classes.ryansafner.com</a><br>'
#date:
output:
  xaringan::moon_reader:
    mathjax: "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_HTMLorMML" # rescales math with css changes https://github.com/yihui/xaringan/issues/143
    lib_dir: libs
    df_print: paged
    #seal: false
    css: [custom.css, custom-fonts.css, "hygge"] #, metropolis, metropolis-fonts
    nature:
      beforeInit: ["macros.js", "https://platform.twitter.com/widgets.js"] # first is for rescaling images , second is for embedding tweets, https://github.com/yihui/xaringan/issues/100
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ratio: "16:9"
      slideNumberFormat: |
        <div class="progress-bar-container">
          <div class="progress-bar" style="width: calc(%current% / %total% * 100%);">
          </div>
        </div>
    includes:
      in_header: header.html # for font awesome, used in title  
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(echo=F,
                      message=F,
                      warning=F)
library(tidyverse)
set.seed(256)
update_geom_defaults("label", list(family = "Fira Sans Condensed"))

library(wooldridge)
wages<-wooldridge::wage1
wages<-wages %>%
  mutate(Gender = factor(ifelse(female==0,
                         "Male",
                         "Female")))
```

# Interaction Effects

- Sometimes one $X$ variable might *interact* with another in determining $Y$

.content-box-green[
.green[**Example**]: Consider the gender pay gap again.

- *Experience* certainly affects wages

- Does experience affect *men*'s wages *differently* than it affects women's wages?
  - i.e. is there an .shout[interaction effect] between gender and experience?

- Note this is *NOT the same* as asking "do men earn more than women *with the same amount of experience*? (i.e. controlling for experience)
]

---

# Three Types of Interactions

- Depending on the types of variables paired, there are three possible types of interaction effects

- We will look at each in turn

--

1. Interaction between a **dummy** and a **continuous** variable:
$$Y_i=\beta_0+\beta_1X_i+\beta_2 D_i+\beta_3 \mathbf{(X_i \times D_i)}$$
--

2. Interaction between a **two dummy** variables:
$$Y_i=\beta_0+\beta_1D_{1i}+\beta_2 D_{2i}+\beta_3 \mathbf{(D_{1i} \times D_{2i})}$$
--

3. Interaction between a **two continuous** variables:
$$Y_i=\beta_0+\beta_1X_{1i}+\beta_2 X_{2i}+\beta_3 \mathbf{(X_{1i} \times X_{2i})}$$

---

class: inverse, center, middle

# Interactions Between a Dummy and Continuous Variable

---

# Interactions Between a Dummy and Continuous Variable I

- We can model an interaction by introducing a variable that is an .shout[interaction term] capturing the interaction between two variables:

$$Y_i=\beta_0+\beta_1X_i+\beta_2 D_i+\beta_3(X_i \times D_i) \quad \text{ where } D_i=\{0,1\}$$

--

- $\beta_3$ estimates the .shout[interaction term] (in this case between a dummy variable and a continuous variable)

--

- What do the different coefficients ($\beta$'s) tell us? 
  - Again, think logically by examining each group $(D_i=0$ or $D_i=1)$

---

# Interaction Effects as Two Regressions I

$$Y_i=\beta_0+\beta_1X_i+\beta_2 D_i+\beta_3 X_i * D_i$$

- .red[When `\\(D_i=0\\)` (Control group):]

$$\begin{align*}
\hat{Y_i}&=\hat{\beta_0}+\hat{\beta_1}X_i+\hat{\beta_2}(\mathbf{0})+\hat{\beta_3}X_i \times (\mathbf{0})\\
\hat{Y_i}& =\hat{\beta_0}+\hat{\beta_1}X_i\\
\end{align*}$$

--

- .blue[When `\\(D_i=1\\)` (Treatment group):]

$$\begin{align*}
\hat{Y_i}&=\hat{\beta_0}+\hat{\beta_1}X_i+\hat{\beta_2}(\mathbf{1})+\hat{\beta_3}X_i \times (\mathbf{1})\\
\hat{Y_i}&= (\hat{\beta_0}+\hat{\beta_2})+(\hat{\beta_1}+\hat{\beta_3})X_i\\
\end{align*}$$

--

- So what we really have is *two* regression lines!

---

# Interaction Effects as Two Regressions II

.pull-left[

```{r, fig.retina=3}
library(mosaic)

control=function(x){2+0.5*x}
treat=function(x){4+x}

ggplot(data.frame(x=c(0,10)), aes(x=x))+
  stat_function(fun=control, geom="line", size=2, color = "red")+
    geom_label(aes(x=5,y=control(5)), color = "red", label=expression(paste("slope=",hat(beta[1]))), size = 5)+
  stat_function(fun=treat, geom="line", size=2, color = "blue")+
    geom_label(aes(x=5,y=treat(5)), color = "blue", label=expression(paste("slope=",hat(beta[1])+hat(beta[3]))), size = 5)+
    scale_x_continuous(breaks=NULL,
                     limits=c(0,10),
                     expand=expand_scale(mult=c(0,0.1)))+
  scale_y_continuous(breaks=c(2,4),
                     labels=c(expression(hat(beta[0])),expression(hat(beta[0])+hat(beta[2]))),
                     limits=c(0,10),
                     expand=expand_scale(mult=c(0,0.1)))+
  labs(x = "",
       y = "")+
  theme_classic(base_family = "Fira Sans Condensed", base_size=20)

```
]

.pull-right[

- .red[`\\(D_i=0\\)` group: `\\(Y_i=\hat{\beta_0}+\hat{\beta_1}X_i\\)`]

- .blue[`\\(D_i=1\\)` group: `\\(Y_i=(\hat{\beta_0}+\hat{\beta_2})+(\hat{\beta_1}+\hat{\beta_3})X_i\\)`]

]

---

# Interpretting Coefficients I

$$Y_i=\beta_0+\beta_1X_i+\beta_2 D_i+\beta_3 \mathbf{(X_i \times D_i)}$$

- To interpret the coefficients, compare cases after changing $X$ by $\mathbf{\Delta X}$:

--

$$Y_i+\mathbf{\Delta Y_i}=\beta_0+\beta_1(X_i\mathbf{+\Delta X_i})\beta_2D_i+\beta_3\big((X_i\mathbf{+\Delta X_i})D_i\big)$$

--

- Subtracting these two equations, the difference is:

--

$$\begin{align*}
	\Delta Y_i &= \beta_1 \Delta X_i + \beta_3 D_i \Delta X_i\\
	\mathbf{\frac{\Delta Y_i}{\Delta X_i}} &\mathbf{= \beta_1+\beta_3 D_i}\\
\end{align*}$$

--

- .onfire[The effect of `\\(X \rightarrow Y\\)` depends on the value of `\\(D_i\\)`!]

- .shout[`\\(\beta_3\\)`: *increment* to the effect of `\\(X \rightarrow Y\\)` when `\\(D_i=1\\)` (vs. `\\(D_i=0\\)`]

---

# Interpretting Coefficients II

$$Y_i=\beta_0+\beta_1X_i+\beta_2 D_i+\beta_3 \mathbf{(X_i \times D_i)}$$

- $\hat{\beta_0}$: $Y_i$ for $X_i=0$ and $D_i=0$

--

- $\beta_1$: Marginal effect of $X_i \rightarrow Y_i$ for $D_i=0$

--

- $\beta_2$: Marginal effect on $Y_i$ of difference between $D_i=0$ and $D_i=1$

--

- $\beta_3$: The **difference** of the marginal effect of $X_i \rightarrow Y_i$ between $D_i=0$ and $D_i=1$

--

- This is a bit awkward, easier to think about the two regression lines: 

---

# Interpretting Coefficients III

$$Y_i=\beta_0+\beta_1X_i+\beta_2 D_i+\beta_3 \mathbf{(X_i \times D_i)}$$

- .red[For `\\(D_i=0\\)` Group:  `\\(\hat{Y_i}=\hat{\beta_0}+\hat{\beta_1}X_i\\)`]
  - Intercept $(Y_i$ for $X_i=0$): $\hat{\beta_0}$
  - Slope (Marginal effect of $X_i \rightarrow Y_i$ for $D_i=0$ group): $\hat{\beta_1}$

--

- .blue[For `\\(D_i=1\\)` Group:  `\\(\hat{Y_i}=(\hat{\beta_0}+\hat{\beta_2})+(\hat{\beta_1}+\hat{\beta_3})X_i\\)`]
  - Intercept $(Y_i$ for $X_i=0$): $\hat{\beta_0}+\hat{\beta_2}$
  - Slope (Marginal effect of $X_i \rightarrow Y_i$ for $D_i=1$ group): $\hat{\beta_1}+\hat{\beta_3}$

--

- How can we determine if the two lines have the same slope and/or intercept?
  - Same intercept? $t$-test $H_0$: $\beta_2=0$
  - Same slope? $t$-test $H_0$: $\beta_3=0$

---

# Example I

.content-box-green[
.green[**Example**]: $$\widehat{wage_i}=\hat{\beta_0}+\hat{\beta_1}exper_i+\hat{\beta_2}female_i+\hat{\beta_3}(exper_i \times female_i)$$

]

--

- .red[For males `\\((female=0)\\)`]:
$$\widehat{wage_i}=\hat{\beta_0}+\hat{\beta_1}exper$$

--

- .blue[For females `\\((female=1)\\)`]:
$$\widehat{wage_i}=\underbrace{(\hat{\beta_0}+\hat{\beta_2})}_{\text{intercept}}+\underbrace{(\hat{\beta_1}+\hat{\beta_3})}_{\text{slope}}exper$$

---

# Example II

.pull-left[

```{r interaction-plot-1, echo=T, eval=F}
interaction_plot<-ggplot(data = wages)+
  aes(x = exper,
      y = wage,
      color = as.factor(female))+ # make female factor
  geom_point()+
  #geom_smooth(method = "lm")+
  scale_y_continuous(labels=scales::dollar)+
  labs(x = "Experience (Years)",
       y = "Wage")+
  guides(color=F)+ # hide legend
  theme_classic(base_family = "Fira Sans Condensed",
           base_size=20)
interaction_plot
```

- Need to make sure `female` is a `factor` variable
  - Use `as.factor()` in plot

]

.pull-right[

```{r, ref.label="interaction-plot-1"}
```
]

---

# Example II

.pull-left[

```{r interaction-plot-2, echo=T, eval=F}
interaction_plot+
  geom_smooth(method="lm")
```
]

.pull-right[

```{r, ref.label="interaction-plot-2"}
```
]

---

# Example II

.pull-left[

```{r interaction-plot-3, echo=T, eval=F}
interaction_plot+
  geom_smooth(method="lm", color = "black")+
  facet_wrap(~female)
```
]

.pull-right[

```{r, ref.label="interaction-plot-3"}
```
]

---

# Example Regression in R I

- Syntax for an interaction term is easy in `R`: `var1:var2`
  - Or could just do `var1*var2` (multiply)

```{r}
interaction_reg<-lm(wage~female+exper+female:exper, data = wages)
```

---

# Example Regression in R II

.font80[
```{r, echo = T}
summary(interaction_reg)
```
]

---

# Example Regression in R III

.pull-left[

```{r huxreg1, echo=T, eval=F}
library(huxtable)
huxreg(interaction_reg,
       coefs = c("Constant" = "(Intercept)",
                 "Experience" = "exper",
                 "Female" = "female",
                 "Female * Exper" = "female:exper"),
       statistics = c("N" = "nobs",
                      "R-Squared" = "r.squared",
                      "SER" = "sigma"),
       number_format = 2)
```

]

.pull-right[

```{r, ref.label="huxreg1"}
```
]

---

# Example Regression in R: Interpretting Coefficients

$$\widehat{wage_i}=6.16+0.05 \, Experience_i - 1.55 \, Female_i - 0.06 \, (Experience_i \times Female_i)$$

--

- $\hat{\beta_0}$: 

---

# Example Regression in R: Interpretting Coefficients

$$\widehat{wage_i}=6.16+0.05 \, Experience_i - 1.55 \, Female_i - 0.06 \, (Experience_i \times Female_i)$$

- $\hat{\beta_0}$: Men with experience of 0 years earn $6.16

--

- $\hat{\beta_1}$:

---

# Example Regression in R: Interpretting Coefficients

$$\widehat{wage_i}=6.16+0.05 \, Experience_i - 1.55 \, Female_i - 0.06 \, (Experience_i \times Female_i)$$

- $\hat{\beta_0}$: Men with experience of 0 years earn $6.16

- $\hat{\beta_1}$: For every additional year of experience, *men* earn $0.05

--

- $\hat{\beta_2}$:

---

# Example Regression in R: Interpretting Coefficients

$$\widehat{wage_i}=6.16+0.05 \, Experience_i - 1.55 \, Female_i - 0.06 \, (Experience_i \times Female_i)$$

- $\hat{\beta_0}$: Men with experience of 0 years earn $6.16

- $\hat{\beta_1}$: For every additional year of experience, *men* earn $0.05

- $\hat{\beta_2}$: *Women* on average earn $1.55 less than men, *holding experience constant*

--

- $\hat{\beta_3}$: 

---

# Example Regression in R: Interpretting Coefficients

$$\widehat{wage_i}=6.16+0.05 \, Experience_i - 1.55 \, Female_i - 0.06 \, (Experience_i \times Female_i)$$

- $\hat{\beta_0}$: Men with experience of 0 years earn $6.16

- $\hat{\beta_1}$: For every additional year of experience, *men* earn $0.05

- $\hat{\beta_2}$: *Women* on average earn $1.55 less than men, *holding experience constant*

- $\hat{\beta_3}$: *Women* earn $0.06 less than men for every additional year of experience

---

# Example Regression in R: Interpretting Coefficients as Two Regressions I

$$\widehat{wage_i}=6.16+0.05 \, Experience_i - 1.55 \, Female_i - 0.06 \, (Experience_i \times Female_i)$$

--

.red[Regression for men `\\(female=0)\\)`]
$$\widehat{wage_i}=6.16+0.05 \, Experience$$

--

- Men with no experience earn $6.16

--

- For every additional year of experience, men earn $0.05 more

---

# Example Regression in R: Interpretting Coefficients as Two Regressions II

$$\widehat{wage_i}=6.16+0.05 \, Experience_i - 1.55 \, Female_i - 0.06 \, (Experience_i \times Female_i)$$

.blue[Regression for women `\\(female=1)\\)`]
$$\begin{align*}
\widehat{wage_i}&=6.16+0.05 \, Experience - 1.55\mathbf{(1)}-0.06 \, Experience \times \mathbf{(1)}\\
&= (6.16-1.55)+(0.05-0.06) \, Experience\\
&= 4.61-0.01 \, Experience \\
\end{align*}$$

--

- Women with no experience earn $4.61

--

- For every additional year of experience, women earn $0.01 *less*

---

# Example Regression in R: Hypothesis Testing

.pull-left[
- Test the significance of interaction effects

- Think: are slopes and intercepts of the two regressions statistically significantly different?

- Are intercepts different?
  - Difference between men vs. women for no experience?
  - Is $\hat{\beta_2}$ significant?
  - Yes: $t = -3.210$, $p$-value = 0.00

- Are slopes different?
  - Difference between men vs. women for marginal effect of experience?
  - Is $\hat{\beta_3}$ significant?
  - Yes: $t = -2.48$, $p$-value = 0.01

]

.pull-right[

```{r, echo=T}
library(broom)
tidy(interaction_reg)
```
]

---

class: inverse, center, middle

# Interactions Between Two Dummy Variables

---

# Interactions Between Two Dummy Variables

$$Y_i=\beta_0+\beta_1D_{1i}+\beta_2 D_{2i}+\beta_3 \mathbf{(D_{1i} \times D_{2i})}$$

- $D_{1i}$ and $D_{2i}$ are dummy variables

--

- $\hat{\beta_1}$: effect on $Y$ of going from $D_{1i}=0$ to $D_{1i}=1$ for $D_{2i}=0$

--

- $\hat{\beta_2}$: effect on $Y$ of going from $D_{2i}=0$ to $D_{2i}=1$ for $D_{1i}=0$

--

- $\hat{\beta_3}$: effect on $Y$ of going from $D_{1i}=0$ and $D_{2i}=1$ to $D_{1i}=1$ *and* $D_{2i}=1$
  - *increment* to the effect of $D_{1i}$ going from 0 to 1 when $D_{2i}=1$ (vs. 0)
  - *increment* to the effect of $D_{2i}$ going from 0 to 1 when $D_{1i}=1$ (vs. 0)

---

- As always, best to think logically about possibilities (when each dummy $=0$ or $=1)$

---

# Interactions Between Two Dummy Variables: Interpretting Coefficients

$$Y_i=\beta_0+\beta_1D_{1i}+\beta_2 D_{2i}+\beta_3 \mathbf{(D_{1i} \times D_{2i})}$$

--

- To interpret coefficients, compare cases (for $D_{i1}$ as an example):

$$\begin{align*}
E(Y_i|D_{1i}=0, D_{2i}=d_2) = \beta_0+\beta_2 d_2\\
E(Y_i|D_{1i}=1, D_{2i}=d_2) = \beta_0+\beta_1(1)+\beta_2 d_2+\beta_3(1)d_2\\
\end{align*}$$

--

- Subtracting the two, the difference is:

$$\beta_1+\beta_3 d_2$$

- .onfire[The effect of `\\(D_{1i} \rightarrow Y_i\\)` depends on the value of `\\(D_{2i}\\)`]
  - $\hat{\beta_3}$ is the *increment* to the effect of $D_1$ on $Y$ when $D_2=1$

---

# Interactions Between Two Dummy Variables: Example

.content-box-green[
.green[**Example**]: Does the gender pay gap change if a person is married vs. single?

$$\widehat{wage_i}=\hat{\beta_0}+\hat{\beta_1}female_i+\hat{\beta_2}married_i+\hat{\beta_3}(female_i \times married_i)$$

]

--

- Logically, there are 4 possible combinations of $female_i = \{0,1\}$ and $married_i = \{0,1\}$

--

1. **Unmarried men** $(female_i=0, \, married_i=0)$
$$\widehat{wage_i}=\hat{\beta_0}$$

--

2. **Married men** $(female_i=0, \, married_i=1)$
$$\widehat{wage_i}=\hat{\beta_0}+\hat{\beta_2}$$

---

# Interactions Between Two Dummy Variables: Example

.content-box-green[
.green[**Example**]: Does the gender pay gap change if a person is married vs. single?

$$\widehat{wage_i}=\hat{\beta_0}+\hat{\beta_1}female_i+\hat{\beta_2}married_i+\hat{\beta_3}(female_i \times married_i)$$

]

- Logically, there are 4 possible combinations of $female_i = \{0,1\}$ and $married_i = \{0,1\}$

3. **Unmarried women** $(female_i=1, \, married_i=0)$
$$\widehat{wage_i}=\hat{\beta_0}+\hat{\beta_1}$$

--

4. **Married women** $(female_i=1, \, married_i=1)$
$$\widehat{wage_i}=\hat{\beta_0}+\hat{\beta_1}+\hat{\beta_2}+\hat{\beta_3}$$

---

# Looking at the Data

.font70[
.pull-left[

```{r, echo = T}
# get average wage for unmarried men
wages %>%
  filter(female == 0,
         married == 0) %>%
  summarize(mean = mean(wage))

# get average wage for married men
wages %>%
  filter(female == 0,
         married == 1) %>%
  summarize(mean = mean(wage))

```
]

.pull-right[

```{r, echo = T}
# get average wage for unmarried women
wages %>%
  filter(female == 1,
         married == 0) %>%
  summarize(mean = mean(wage))

# get average wage for married women
wages %>%
  filter(female == 1,
         married == 1) %>%
  summarize(mean = mean(wage))

```
]
]

---

# Interactions Between Two Dummy Variables: Group Means

$$\widehat{wage_i}=\hat{\beta_0}+\hat{\beta_1}female_i+\hat{\beta_2}married_i+\hat{\beta_3}(female_i \times married_i)$$

| Gender | Unmarried | Married |
|--------|-----------|---------|
| Male   | $5.17     | $7.98   |
| Female | $4.61     | $4.57   |

---

# Interactions Between Two Dummy Variables: In R I

.font70[
```{r, echo=T}
reg_dummies<-lm(wage~female+married+female:married, data = wages)
summary(reg_dummies)
```
]

---

# Interactions Between Two Dummy Variables: In R II

.pull-left[

```{r huxreg2, echo=T, eval=F}
library(huxtable)
huxreg(reg_dummies,
       coefs = c("Constant" = "(Intercept)",
                 "Female" = "female",
                 "Married" = "married",
                 "Female * Married" = "female:married"),
       statistics = c("N" = "nobs",
                      "R-Squared" = "r.squared",
                      "SER" = "sigma"),
       number_format = 2)
```

]

.pull-right[

```{r, ref.label="huxreg2"}
```
]

---

# Interactions Between Two Dummy Variables: Interpretting Coefficients

$$\widehat{wage_i}=5.17-0.56 \, female_i + 2.82 \, married_i - 2.86 \, (female_i \times married_i)$$

| Gender | Unmarried | Married |
|--------|-----------|---------|
| Male   | $5.17     | $7.98   |
| Female | $4.61     | $4.57   |

--

- Wage for **unmarried men**: $\hat{\beta_0}=5.17$

--

- Wage for **married men**: $\hat{\beta_0}+\hat{\beta_2}=5.17+2.82=7.98$

--

--

- Wage for **unmarried women**: $\hat{\beta_0}+\hat{\beta_1}=5.17-0.56=4.61$

--

- Wage for **married women**: $\hat{\beta_0}+\hat{\beta_1}+\hat{\beta_2}+\hat{\beta_3}=5.17-0.56+2.82-2.86=4.57$

---

class: inverse, center, middle

# Interactions Between Two Continuous Variables

---

# Interactions Between Two Continuous Variables

$$Y_i=\beta_0+\beta_1X_{1i}+\beta_2 X_{2i}+\beta_3 \mathbf{(X_{1i} \times X_{2i})}$$

--

- To interpret coefficients, compare changes after changing $\mathbf{\Delta X_{1i}}$:

$$Y+\mathbf{\Delta Y}=\beta_0+\beta_1(X_1+\mathbf{\Delta X_1})\beta_2X_2+\beta_3((X_1+\mathbf{\Delta X_1}) \times X_2)$$

--

- Take the difference to get:

--

$$\begin{align*}
\Delta Y &= \beta_1 \Delta X_1 + \beta_3 X_2 \Delta X_1\\
\frac{\Delta Y}{\Delta X_1} &= \beta_1+\beta_3 X_2\\ 	
\end{align*}$$

--

- .onfire[The effect of `\\(X_1 \rightarrow Y_i\\)` depends on `\\(X_2\\)`]

- $\beta_3$: increment to the effect of $X_1 \rightarrow Y_i$ from a 1 unit change in $X_2$

---

# Interactions Between Two Continuous Variables: Example

.content-box-green[
.green[**Example**]: Do education and experience interact in their determination of wages?

$$	\widehat{wage_i}=\hat{\beta_0}+\hat{\beta_1} \, educ_i+\hat{\beta_2} \, exper_i+\hat{\beta_3}(educ_i \times exper_i)$$

]

- Estimated effect of education on wages depends on the amount of experience (and vice versa)!

$$\frac{\Delta wage}{\Delta educ}=\hat{\beta_1}+\beta_3 \, exper$$

- This is a type of nonlinearity (we will examine nonlinearities next lesson)

---

# Interactions Between Two Continuous Variables: In R I

.font70[
```{r, echo=T}
reg_cont<-lm(wage~educ+exper+educ:exper, data = wages)
summary(reg_cont)
```
]

---

# Interactions Between Two Continuous Variables: In R II

.pull-left[

```{r huxreg3, echo=T, eval=F}
library(huxtable)
huxreg(reg_cont,
       coefs = c("Constant" = "(Intercept)",
                 "Education" = "educ",
                 "Experience" = "exper",
                 "Education * Experience" = "educ:exper"),
       statistics = c("N" = "nobs",
                      "R-Squared" = "r.squared",
                      "SER" = "sigma"),
       number_format = 3)
```

]

.pull-right[

```{r, ref.label="huxreg3"}
```

]

---

# Interactions Between Two Continuous Variables: Interpretting Coefficients

$$\widehat{wages_i}=-2.860+0.602 \, educ_i + 0.047 \, exper_i + 0.002 \, (educ_i \times exper_i)$$

--

Changes in Education: 

| Experience | $\displaystyle\frac{\Delta wage}{\Delta educ}$ |
|------------|------------------------------------------------|
| 5 years | $0.602+0.002(5)=0.612$ |
| 10 years | $0.602+0.002(10)=0.622$ |
| 15 years | $0.602+0.002(15)=0.632$ |

--

- Marginal effect of education $\rightarrow$ wages **increases** with more experience (but very insignificantly)